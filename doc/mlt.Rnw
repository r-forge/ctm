
\documentclass[article,nojss,shortnames]{jss}

%% packages
\usepackage{thumbpdf}
\usepackage{amsfonts,amstext,amsmath,amssymb,amsthm}
\usepackage{accents}
\usepackage{color}
\usepackage{rotating}
\usepackage{verbatim}
%% need no \usepackage{Sweave.sty}
%%\usepackage[nolists]{endfloat}

\newcommand{\cmd}[1]{\texttt{#1()}}

<<setup, echo = FALSE, results = "hide", message = FALSE>>=
printfun <- function(fun) {
    f <- formals(fun)
    cf <- sapply(f, function(x) {
        if (is.call(x)) return(parse(text = x))
        if (is.null(x)) return("NULL")
        return(as.character(x))
    })
    fun <- as.name(match.call()[[2]])
    ret <- paste(fun, "(", paste(names(f), ifelse(cf == "", "", " = ") , 
          cf, collapse = ", ", sep = ""), ")", sep = "")
    cat(ret, "\n")
}
library("mlt")
library("survival")
library("eha")
library("prodlim")
library("truncreg")
library("lattice")
library("gridExtra")
library("MASS")
library("HSAUR3")
library("grid") ### for textGrob
library("latticeExtra")
trellis.par.set(list(plot.symbol = list(col=1,pch=20, cex=0.7),
                     box.rectangle = list(col=1),
                     box.umbrella = list(lty=1, col=1),
                     strip.background = list(col = "white")))
ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)
library("colorspace")
library("multcomp")
logLik.phreg <- function(object) object$loglik[2]
vcov.phreg <- function(object) object$var
opts_chunk$set(warning = FALSE, message = FALSE, fig.width=4, fig.height=4)
@

\newcommand{\TODO}[1]{{\color{red} #1}}

\newcommand\Torsten[1]{{\color{blue}Torsten: ``#1''}}

% File with math commands etc.
\input{defs.tex}

\renewcommand{\thefootnote}{}

%% code commands
\newcommand{\Rclass}[1]{`\code{#1}'}
%% JSS
\author{Torsten Hothorn \\ Universit\"at Z\"urich}
\Plainauthor{Hothorn}

\title{\pkg{mlt}: Transformation Analysis in R} 
\Plaintitle{mlt: Transformation Analysis in R}
\Shorttitle{Transformation Analysis}

\Abstract{
tbd
}

\Keywords{transformation model, distribution regression, conditional
distribution function, conditional quantile function, censoring,
truncation}
\Plainkeywords{transformation model, distribution regression, conditional
distribution function, conditional quantile function, censoring,
truncation}

\Address{
  Torsten Hothorn\\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Universit\"at Z\"urich \\
  Hirschengraben 84, CH-8001 Z\"urich, Switzerland \\
  \texttt{Torsten.Hothorn@uzh.ch} \\
}

\begin{document}

\input{todo}

\section{Introduction}

The history of statistics can be told as a story of great conceptual ideas
and contemporaneous computable approximations thereof.  As time went by, the
computationally inaccessible concept often vanished from the collective
consciousness of our profession and the approximation was taught and
understood as the real thing.  Least squares regression emerged from
Gau{\ss}' computational trick of changing Bo\v{s}covi{\'c}' absolute to squared
error and it took more than 200 years for the original, and in many aspects
advantageous, concept to surface again under the name ``quantile
regression''.  This most prominent example of an idea got lost illustrates
the impact computable approximations had and still have on our understanding
of statistical methods and procedures.  In the early days of statistical
computing, implementations of such approximations were a challenge.  With
todays computing power and software infrastructure at our fingertips, our
duty shall be to go back to the original concepts and search for ways how to
reawake them for the benefit of a simpler understanding of statistical
models and concepts.

This paper describes an attempt to understand and unify a large class of
statistical models as models for distributions.  This sounds like an
implicitness, but do we really practice (in courses on applied statistics or
while talking to our subject-matter collaborators) what we preach in a
theory course?  Let's perform a small experiment: Pick, at random, a
statistics book from your book shelf and look-up how the general linear
model is introduced. Most probably you will find something not unlike
\begin{eqnarray*}
\rY = \alpha + \rx^\top \shiftparm + \varepsilon, \quad \varepsilon \sim \ND(0, \sigma^2)
\end{eqnarray*}
where model interpretation relies on $\Ex(\rY | \rX = \rx) = \alpha + \rx^\top \shiftparm$ and
one estimates the intercept $\alpha$ and the 
regression parameters $\shiftparm$ by minimisation of the squared error 
$(\rY - \alpha - \rx^\top \shiftparm)^2$. With some touch-up in notation, the model 
can be equivalently written as a model for a conditional distribution
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \Phi\left(\frac{\ry - \alpha - \rx^\top \shiftparm}{\sigma}\right) \text{ or } (\rY | \rX = \rx) \sim \ND(\alpha + \rx^\top\shiftparm, \sigma^2).
\end{eqnarray*}
This little change highlights that the model is, in fact, a model for a
conditional distribution and not just a model for a conditional mean. It also
stresses the fact that the variance $\sigma^2$ is a model parameter in its own right. The usual
treatment of $\sigma^2$ as a nuisance parameter only works when the likelihood 
is approximated by the density of the normal distribtion. Because we always observe
intervals $(\ubar{\ry}, \bar{\ry}]$ and never real numbers $\ry$, the exact likelihood
is, as originally \textit{defined} by \cite{Fisher_1934} \citep[another example of the approximation winning over the basic concept][]{Lindsey_1996}
\begin{eqnarray*}
\Prob(\ubar{\ry} < \rY \le \bar{\ry} | \rX = \rx) = 
\Phi\left(\frac{\bar{\ry} - \alpha - \rx^\top \shiftparm}{\sigma}\right) - \Phi\left(\frac{\ubar{\ry} - \alpha - \rx^\top \shiftparm}{\sigma}\right)
\end{eqnarray*}
which requires simultaneous optimisation of all three model parameters $\alpha$, $\shiftparm$ and $\sigma$ but
is exact also under other forms of random censoring. If we were going to reformulate the model
a little further to
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \Phi(\tilde{\alpha}_1 + \tilde{\alpha}_2 \ry - \rx^\top \tilde{\shiftparm})
\end{eqnarray*}
with $\tilde{\alpha}_1 = -\alpha / \sigma, \tilde{\alpha}_2 = 1 / \sigma$ and $\tilde{\shiftparm} = \shiftparm / \sigma$
we see that the model is of the form
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h_\rY(\ry) - \rx^\top \tilde{\shiftparm})
\end{eqnarray*}
with distribution function $\pZ = \Phi$ and linear transformation $\h_\rY(\ry) = \tilde{\alpha}_1 + \tilde{\alpha}_2 \ry$
such that $\Ex(\h_\rY(\rY) | \rX = \rx) = \rx^\top \shiftparm$. If we now change $\pZ$ to the distribution
function of the minimum extreme value distribution and allow a non-linear monotone transformation
$\h_\rY$ we get
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = 1 - \exp(-\exp(\h_\rY(\ry) - \rx^\top \tilde{\shiftparm}))
\end{eqnarray*}
which is the continuous proportional hazards, or Cox, model. From this point
of view, the linear and the Cox model are two instances of so-called linear
transformation models (a misleading name, because the transformation
$\h_\rY$ is non-linear in the latter case and only the shift $\rx^\top
\tilde{\shiftparm}$ is linear in $\rx$).  It is now also obvious that the Cox
model has nothing to do with censoring, let alone survival times $\rY > 0$. 
It is a model for the conditional distribution of a continuous responses
$\rY \in \RR$ when it is appropriate to assume that the conditional hazard
function is scaled by $\exp(\rx^\top \tilde{\shiftparm})$.  For both the linear
and the Cox model, application of the exact likelihood allows the models to
be fitted to imprecise, or ``censored'', observations $(\ubar{\ry},
\bar{\ry}]$.

The class of linear transformation models is a subclass of conditional 
transformation models. In this latter class, the conditional distribution function is 
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h(\ry | \rx))
\end{eqnarray*}
where the transformation function $\h$ depends on both $\ry$ and $\rx$. Because
we are interested in analysing, \ie estimating and interpreting, the transformation
function $\h$, we refer to the methods presented in this paper as \textit{transformation analysis}.
We describe how conditional transformation models can be specified, fitted and analysed in \proglang{R}
using the \pkg{mlt} add-on package. Before we start looking at details,
we illustrate the workflow by means of an example from unconditional density 
estimation.

\paragraph{Density Estimation: Old Faithful Geyser Data}

The duration of eruptions and the waiting time between eruptions of the Old
Faithful geyser in the Yellowstone national park became a standard benchmark
for non-parametric density estimation \citep[the original data were given
by][]{Azzalini_Bowman_1990}.  An unconditional density estimate for the
duration of the eruptions needs to deal with censoring because exact
duration times are only available for the day time measurements.  At night
time, the observations were either left-censored (``short'' eruption),
interval-censored (``medium'' eruption) or right-censored (``long''
eruption) as explained by \cite{Azzalini_Bowman_1990}.  This fact was widely
ignored in analyses of the Old Faithful data because most non-parametric
kernel techniques cannot deal with censoring. We fit the parameters
$\parm$ in the transformation model
\begin{eqnarray*}
\Prob(\rY \le \ry) = \Phi(\h(\ry)) = \Phi(\basisy(\ry)^\top \parm)
\end{eqnarray*}
by maximisation of the exact likelihood as follows. After loading package
\pkg{mlt} we specify the \code{duration} variable we are interested in
<<geyser-var, echo = TRUE>>=
dvar <- numeric_var("duration", support = c(1.0, 5.0), 
                    add = c(-1, 1), bound = c(0, Inf))
@
This abstract representation refers to a positive and conceptually
continuous variable \code{duration} and is explained in more detail in Appendix~\ref{app:variables}.  
We then set-up a basis function
$\basisy$ for this variable in the interval $[1, 5]$, in our case a monotone
increasing Bernstein polynomial of order eight
<<geyser-basis, echo = TRUE>>=
B_d <- Bernstein_basis(order = 8, var = dvar, ui = "incre")
@
This and other basis functions are described in Appendix~\ref{app:basefun}.
The (in our case unconditional) transformation model is now fully defined by
the parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$ and $\pZ = \Phi$
which will be discussed in Section~\ref{sec:trafo}
<<geyser-ctm, echo = TRUE>>=
ctm_d <- ctm(response = B_d, todistr = "Normal")
@
and we can generate a grid of duration times from
<<geyser-grid, echo = TRUE>>=
str(ndd <- mkgrid(ctm_d, 200))
@
Only after the model was specified we need to load the data frame containing the
observations of \code{duration} as a \code{Surv} object: 
<<geyser-data, echo = TRUE>>=
data("geyser", package = "TH.data")
head(geyser)
@
The most likely transformation (MLT) $\hat{\h}(\ry) = \basisy(\ry)^\top \hat{\parm}$ is 
now obtained from the maximum likelihood estimate $\hat{\parm}$ computed as
<<geyser-fit, echo = TRUE>>=
mlt_d <- mlt(ctm_d, data = geyser, check = FALSE)
logLik(mlt_d)
coef(mlt_d)
@
Details on the estimation procedure will be presented in Section~\ref{sec:mlt}.
The model is best visualised in terms of the corresponding density
$\phi(\basisy(\ry)^\top \hat{\parm}) \basisy^\prime(\ry)^\top \hat{\parm}$,
extracted from the fitted model as
<<geyser-density, echo = TRUE>>=
ndd$d <- predict(mlt_d, newdata = ndd, type = "density")
@
Model inspection by various ways at looking at the estimated transformation
function will by introduced in Section~\ref{sec:predict}.  The result is
depicted in Figure~\ref{fig:geyser-plot}.  The plot shows the well-known
bimodal distribution in a nice smooth way.  Several things are quite unusual
in this short example.  First, the model was specified without reference to
the actual observations, maybe with the exception of the domain of the
Bernstein polynomial (Section~\ref{sec:trafo}).  Second, although the model
is fully parametric, the resulting density resembles a non-parametric
flexibility.  Third, the exact likelihood as defined by the
interval-censored observations, was used to obtain the model
(Section~\ref{sec:mlt}).  Fourth, inspection of the parameter estimates is
uninteresting, the model is better looked at by means of the estimated
distribution, density, quantile, hazard or cumulative hazard functions
(Section~\ref{sec:predict}).  Fifth, no regularisation was necessary due to
the monotonicity constraint (implemented as linear constraints for maximum
likelihood estimation) and thus standard likelihood asymptotics work for
$\hat{\parm}$ (Section~\ref{sec:asympt}).  Sixth, because the model is a
model for a full distribution, we can easily draw random samples from the
model and refit its parameters using the parametric or model-based bootstrap
(Section~\ref{sec:sim}).  Seventh, all of this is not only possible
theoretically but readily implemented in package \pkg{mlt}.  The only
remaining question is ``Do all this nice properties carry over to the
conditional case, \ie to regression models?''.  The answer to this question
is ``yes!'' and the rest of this paper describes the details following the
workflow sketched in this section.

\begin{figure}
\begin{center}
<<geyser-plot, echo = FALSE, fig.width = 5, fig.height = 4>>=
plot(d ~ duration, data = ndd, type = "l", ylab = "Density", xlab = "Duration time")
@
\caption{Old Faithful Geyser. Estimated density for duration time obtained from
         an unconditional transformation model where the transformation function
         was parameterised by means of a Bernstein polynomial. The plot reproduces
         Figure~1 (right panel) in \cite{Hothorn_Moest_Buehlmann_2016}. \label{fig:geyser-plot}}
\end{center}
\end{figure}

 
\section{Specifying Transformation Models} \label{sec:trafo}

In this section we introduce a cascade of increasingly complex
transformation models and discuss how one can specify such models using the
infrastructure provided by the \pkg{mlt} package.  The models are
illustrated by a applications from different domains.  We start with the
simplest case of models for unconditional distribution functions.

\subsection{Unconditional Transformation Models}

The distribution function on an at least ordered response $\rY$ is
defined in terms of a transformation function $\h$ and a distribution
function $\pZ$. The transformation function is parameterised in terms
of a basis function $\basisy$:
\begin{eqnarray*}
\Prob(\rY \le \ry) = \pZ(\h(\ry)) = \pZ(\basisy(\ry)^\top \parm).
\end{eqnarray*}
The triple $(\pZ, \basisy, \parm)$ fully defines the distribution of $\rY$ and
is called transformation model.
The choice of the basis function $\basisy$ depends on the measurement 
scale of $\rY$ and we can differentiate between the following situations.

\subsubsection{Discrete Models for Categorical Responses}

For ordered categorical responses $\rY$ from a
finite sample space $\samY = \{\ry_1, \dots, \ry_K\}$ the distribution function
$\pY$ is a step-function with jumps at $\ry_k$ only. We therefore
assign one parameter to each jump, \ie each element of the sample space except $\ry_K$.  This
corresponds to the basis function $\basisy(\ry_k) = \evec_{K - 1}(k)$, where
$\evec_{K-1}(k)$ is the unit vector of length $K - 1$ with its
$k$th element being one.  The transformation function $\h$ is
\begin{eqnarray*}
\h(\ry_k) & = & \evec_{K - 1}(k)^\top \parm = \eparm_k \in \RR, \quad 1 \le k < K, \quad
\text{st} \quad \eparm_1 < \dots <
\eparm_{K - 1}
\end{eqnarray*}
with $\h(\ry_K) = \infty$,  and the unconditional distribution function of $\pY$ is
$\pY(\ry_k) = \pZ(\eparm_k)$. Note that monotonicity of $\h$ is guaranteed by the
$K - 2$ linear constraints $\eparm_2 - \eparm_1 > 0, \dots, \eparm_{K -1} -
\eparm_{K -2} > 0$ when constrained optimisation is performed.

\paragraph{Categorical Data Analysis: Chinese Health and Family Life Survey}

The Chinese Health and Family Life Survey \citep{Parish}, conducted 1999--2000 as a
collaborative research project of the Universities of Chicago, Beijing, and
North Carolina, sampled $60$ villages and urban neighborhoods in China. 
Eighty-three individuals were chosen at random for each location from
official registers of adults aged between $20$ and $64$ years to target a
sample of $5000$ individuals in total.  Here, we restrict our attention to
women with current male partners for whom no information was missing,
leading to a sample of $\Sexpr{nrow(CHFLS)}$ women with the following
variables: \code{R\_edu} (level of education of the responding woman),
\code{R\_income} (monthly income in yuan of the responding woman),
\code{R\_health} (health status of the responding woman in the last year) and
\code{R\_happy} (how happy was the responding woman in the last year).
We first estimate the unconditional distribution of happiness using a
proportional odds model \citep[\cmd{polr} from package \pkg{MASS}][]{pkg:MASS}
<<CHFLS-1>>=
data("CHFLS", package = "HSAUR3")
polr_CHFLS_1 <- polr(R_happy ~ 1, data = CHFLS)
@
The basis function introduced above corresponds to a model matrix for
the ordered factor \code{R\_happy} with treatment contrasts using the largest level
(``very happy'') as baseline group. In addition, the parameters must satisfy
the linear constraint $\mA \parm \ge \mvec$, $\mA$ (argument \code{ui}) 
being the difference matrix and $\mvec = \bold{0}$ (argument \code{ci})
<<CHFLS-1-basefun>>=
nl <- nlevels(CHFLS$R_happy)
b_happy <- as.basis(~ R_happy, data = CHFLS, remove_intercept = TRUE,
                    contrasts.arg = list(R_happy = function(n) 
                        contr.treatment(n, base = nl)),
                    ui = diff(diag(nl - 1)), ci = rep(0, nl - 2))
@
We are now ready to set-up the (unconditional) transformation model by a call to
\cmd{ctm} using the basis function and a character defining the standard 
logistic distribution function for $\pZ$:
<<CHFLS-1-ctm>>=
ctm_CHFLS_1 <- ctm(response = b_happy, todist = "Logistic")
@
Note that the choice of $\pZ$ is completely arbitrary as the estimated distribution
function is invariant with respect to $\pZ$.
The model is fitted by calling the \cmd{mlt} function with model and data as arguments
<<CHFLS-1-mlt>>=
mlt_CHFLS_1 <- mlt(ctm_CHFLS_1, data = CHFLS, check = FALSE)
@
The results are equivalent to the results obtained from \cmd{polr}:
<<CHFLS-1-cmpr>>=
logLik(polr_CHFLS_1)
logLik(mlt_CHFLS_1)
cbind(polr = polr_CHFLS_1$zeta, mlt = coef(mlt_CHFLS_1))
cbind(polr = sqrt(diag(vcov(polr_CHFLS_1))),
      mlt = sqrt(diag(vcov(mlt_CHFLS_1))))
@
Of course, the above exercise is an extremely cumbersome way
of estimating a discrete density those maximum likelihood estimator
simply is the relative frequency but, as we will see in the rest of this paper, a generalisation
to the conditional case strongly relies on this parameterisation
<<CHFLS-1-pred>>=
predict(polr_CHFLS_1, newdata = data.frame(1), type = "prob")
### predict(mlt_CHFLS_1, newdata = data.frame(1), type = "density")
xtabs(~ R_happy, data = CHFLS) / nrow(CHFLS)
@

\subsubsection{Continuous Models for Continuous Responses}

For continuous responses $\rY$ the
parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$, and thus also
$\hatpY$, should be smooth in $\ry$, so any polynomial or spline basis is a
suitable choice for $\basisy$.  We apply Bernstein polynomials \citep[for an
overview see][]{Farouki_2012} of order $M$
($\dimparm = M + 1$) defined on the interval $[\ubar{\imath}, \bar{\imath}]$ with
\begin{eqnarray*}
\bern{M}(\ry) & = & (M + 1)^{-1}(f_{\text{Be}(1, M + 1)}(\tilde{\ry}), \dots,
                            f_{\text{Be}(m, M - m + 1)}(\tilde{\ry}), \dots,
                            f_{\text{Be}(M + 1, 1)}(\tilde{\ry}))^\top \in \RR^{M + 1} \\
\h(\ry) & = & \bern{M}(\ry)^\top \parm =
              \sum_{m = 0}^{M} \eparm_m f_{\text{Be}(m + 1, M - m + 1)}(\tilde{\ry}) / (M + 1) \\
\h^\prime(\ry) & = & \bern{M}^\prime(\ry)^\top \parm =
              \sum_{m = 0}^{M - 1} (\eparm_{m + 1} - \eparm_m) f_{\text{Be}(m + 1, M - m)}(\tilde{\ry}) M /
((M + 1) (\bar{\imath} - \ubar{\imath}))
\end{eqnarray*}
where $\tilde{\ry} = (\ry -\ubar{\imath}) / (\bar{\imath} - \ubar{\imath}) \in [0,
1]$ and $f_{\text{Be}(m, M)}$ is the density of the Beta distribution with
parameters $m$ and $M$.  This choice is computationally attractive because
strict monotonicity can be formulated as a set of $M$ linear constraints on the
parameters $\eparm_m < \eparm_{m + 1}$ for all $m = 0, \dots, M$
\citep{Curtis_Ghosh_2011}.  Therefore, application of constrained optimisation guarantees
monotone estimates $\hat{\h}_N$. The basis contains an intercept.  

\paragraph{Density Estimation: Geyser Data}
We continue the analysis of the Old Faithful data by estimating the unconditional distribution of
waiting times, a positive variable those abstract representation can be used to generate
an equidistant grid of $100$ values
<<geyser-w>>=
wvar <- numeric_var("waiting", support = c(40.0, 100), add = c(-5, 15), 
                    bound = c(0, Inf))
c(sapply(ndw <- mkgrid(wvar, 100), range))
@
A monotone increasing Bernstein polynomial of order $8$ for \code{waiting} is defined as
<<geyser-bernstein>>=
B_w <- Bernstein_basis(wvar, order = 8, ui = "increasing")
@
The (in again unconditional) transformation model is now fully described by
the parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$ and $\pZ = \Phi$, the latter 
choice again being not important)
<<geyser-w-ctm, echo = TRUE>>=
ctm_w <- ctm(response = B_w, todistr = "Normal")
@
The most likely transformation $\hat{\h}(\ry) = \basisy(\ry)^\top \hat{\parm}$ is 
now obtained from the maximum likelihood estimate $\hat{\parm}$ computed as
<<geyser-w-fit, echo = TRUE>>=
mlt_w <- mlt(ctm_w, data = geyser, check = FALSE)
@
and we compare the estimated distribution function 
<<geyser-w-distribution, echo = TRUE>>=
ndw$d <- predict(mlt_w, newdata = ndw, type = "distribution")
@
with the empirical cumulative distribution function (the non-parametric maximum likelihood estimator) 
in the left panel of Figure~\ref{fig:geyser-w-plot}.

\begin{figure}
\begin{center}
<<geyser-w-plot, echo = FALSE, cache = TRUE, fig.height = 4, fig.width = 6, results = "hide">>=
layout(matrix(1:2, ncol = 2))
plot(ecdf(geyser$waiting), col = "grey", xlab = "Waiting times", ylab = "Distribution", 
     main = "")
lines(ndw$waiting, ndw$d)
B_w_40 <- Bernstein_basis(order = 40, var = wvar, ui = "incre")
ctm_w_40 <- ctm(B_w_40, todistr = "Normal")
mlt_w_40 <- mlt(ctm_w_40, data = geyser, check = FALSE)
ndw$d2 <- predict(mlt_w_40, q = ndw$waiting, type = "distribution")
lines(ndw$waiting, ndw$d2, lty = 2)
legend("bottomright", lty = 1:2, legend = c("M = 8", "M = 40"), bty = "n")
plot(ndw$waiting, predict(mlt_w, q = ndw$waiting, type = "density"), type = "l",
     ylim = c(0, .04), xlab = "Waiting times", ylab = "Density")
lines(ndw$waiting, predict(mlt_w_40, q = ndw$waiting, type = "density"), lty = 2)
rug(geyser$waiting, col = rgb(.1, .1, .1, .1))
@
\caption{Old Faithful Geyser. Estimated distribution (left, also featuring the empirical
         cumulative distribution function) and density (right) function for 
         two transformation models parameterised in terms of Bernstein polynomials
         of order $8$ and $40$.  The right plot reproduces the density for $M = 8$ shown in
         Figure~1 (left panel) in \cite{Hothorn_Moest_Buehlmann_2016}. \label{fig:geyser-w-plot}}
\end{center}
\end{figure}

The question arises how the degree of the polynomial affects the estimated
distribution function.  On the one hand, the model $(\Phi, \bern{1}, \parm)$ only allows
linear transformation functions of a standard normal and $\pY$ is restricted to the
normal family.  On the other hand, $(\Phi, \bern{N - 1}, \parm)$ has one
parameter for each observation and $\hatpY$ is the non-parametric maximum
likelihood estimator $\text{ECDF}$ which, by the Glivenko-Cantelli lemma,
converges to $\pY$.  In this sense, we cannot choose $M$ ``too large''. This
is a consequence of the monotonicity constraint on the estimator
$\basisy^\top \hat{\parm}_N$ which, in this extreme case, just interpolates
the step-function $\pZ^{-1} \circ \text{ECDF}$. The practical effect can be inspected
in Figure~\ref{fig:geyser-w-plot} where two Bernstein polynomials of order $M = 8$ and
$M = 40$ are compared on the scale of the distribution function (left panel) and density
function (right panel). It is hardly possible to notice the difference in probabilities
but the more flexible model features a more erratic density estimate, but not overly so.
The corresponding AIC values are $\Sexpr{round(AIC(mlt_w),1)}$ 
for $M = 8$ and $\Sexpr{round(AIC(mlt_w_40),1)}$ for $M = 40$ favoring the smaller model.


\subsection{Linear Transformation Models}

A linear transformation model features a linear shift of a (usually) non-linear transformation
of the response $Y$ and is the simplest form of a regression model in the class of
conditional transformation models. The conditional distribution function is
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h(\ry | \rx)) = 
\pZ(\h(\ry) - \rx^\top \shiftparm).
\end{eqnarray*}
The transformation $\h$, sometimes called ``baseline transformation'' can be parameterised
as discussed in the unconditional situation:
\begin{eqnarray*}
\pZ(\h(\ry) - \rx^\top \shiftparm) = \pZ(\basisy(\ry)^\top \parm_1 - \rx^\top \shiftparm)
\end{eqnarray*}
The connection to more complex models is highlighted by the introduction of a 
basis function $\basisyx$ being conditional on the explanatory variables $\rx$, \ie
\begin{eqnarray*}
\pZ(\h(\ry | \rx)) = \pZ(\basisyx(\ry, \rx)^\top \parm) = \pZ(\basisy(\ry)^\top \parm_1 - \rx^\top \shiftparm) \, \text{with} \, \basisyx = (\basisy^\top, -\rx^\top)^\top.
\end{eqnarray*}
The definition of a linear transformation model only requires the basis $\basisy$, 
the explanatory variables $\rx$ and a distribution $\pZ$. The latter choice is now important,
as additivity of the linear predictor is assumed on the scale of $\pZ$. It is convenient to
supply negative linear predictors as $\Ex(\h(\rY) | \rX = \rx) = \rx^\top \shiftparm$.

\subsubsection{Discrete Response}

\paragraph{Categorical Data Analysis: Chinese Survey} We want to study the impact of
age and income on happiness in a proportional odds model, here fitted using \cmd{polr} first
<<CHFLS-2, cache = TRUE>>=
polr_CHFLS_2 <- polr(R_happy ~ R_age + R_income, data = CHFLS)
@
In addition to the basis function for happiness (\code{b\_happy}) we need to 
set-up the basis function for a negative linear predictor without intercept (which is already
included in \code{b\_happy})
<<CHFLS-2-base>>=
b_R <- as.basis(~ R_age + R_income, data = CHFLS, remove_intercept = TRUE, 
                   negative = TRUE)
@
The model is now defined by two basis functions, one of the response and one for the
shift in addition to $\pZ$ and fitted using the \cmd{mlt} function
<<CHFLS-2-ctm>>=
ctm_CHFLS_2 <- ctm(response = b_happy, shift = b_R, todist = "Logistic")
mlt_CHFLS_2 <- mlt(ctm_CHFLS_2, data = CHFLS, check = FALSE, scale = TRUE)
@
Again, the results of \cmd{polr} and \cmd{mlt} are equivalent
<<CHFLS-2-cmpr>>=
logLik(polr_CHFLS_2)
logLik(mlt_CHFLS_2)
cbind(polr = c(polr_CHFLS_2$zeta, coef(polr_CHFLS_2)), 
      mlt = coef(mlt_CHFLS_2))
#cbind(polr = sqrt(diag(vcov(polr_CHFLS_2)))[c(3, 4, 1, 2)],
#      mlt = sqrt(diag(vcov(mlt_CHFLS_2))))
@
The regression coefficients $\shiftparm$ are the log-odds ratios, \ie 
the odds-ratio between any two subsequent happiness categories is
$\Sexpr{round(exp(coef(mlt_CHFLS_2)["R_age"]), 4)}$ for each year of age and
$\Sexpr{round(exp(coef(mlt_CHFLS_2)["R_income"]), 4)}$ for each additional Yuan earned,
only the latter positive effect of income being ``significant'' by standard measures
<<CHFLS-2-cftest>>=
cftest(mlt_CHFLS_2)
@

\subsubsection{Continuous Responses}

\paragraph{Survival Analysis: German Breast Cancer Study Group-2 (GBSG-2) Trial} 

This prospective, controlled clinical trial on the treatment of node positive
breast cancer patients was conducted by the German Breast Cancer Study 
Group \citep[GBSG-2,][]{gbsg2:1994}.  Patients not older than $65$ years with
positive regional lymph nodes but no distant metastases were included in
the study.  Out of $686$ women, $246$ received hormonal therapy whereas the
control group of $440$ women did not receive hormonal therapy.  Additional
variables include age, menopausal status, tumor size, tumor grade, number of
positive lymph nodes, progesterone receptor and estrogen receptor.  The
right-censored recurrence-free survival time is the response variable of
interest.

First, we fitted the Cox model $(\pMEV, (\bern{5}^\top, \tilde{\rx}^\top)^\top,
\parm)$ to the data, where $\tilde{\rx}$ contains the treatment indicator
and all other variables in the transformation function $\h(\ry | \rx) =
\bern{5}(\ry)^\top \parm_1 + \tilde{\rx}^\top \shiftparm$.  In contrast to the
classical Cox model where only $\shiftparm$ is estimated by the partial
likelihood, we estimated all model parameters simultaneously under five linear constraints.  
The results obtained for $\shiftparm$ from the partial and the exact log-likelihood are practically
equivalent:

<<GBSG2-1, echo = TRUE>>=
data("GBSG2", package = "TH.data")
GBSG2$y <- with(GBSG2, Surv(time, cens))
GBSG2y <- numeric_var("y", support = c(100.0, max(GBSG2$time)), bounds = c(0, Inf))
### Cox model
B_GBSG2y <- Bernstein_basis(var = GBSG2y, order = 10, ui = "incre")
fm_GBSG2 <- Surv(time, cens) ~ horTh + age + menostat + tsize + tgrade + pnodes + progrec + estrec
ctm_GBSG2_1 <- ctm(B_GBSG2y, shift = fm_GBSG2[-2L], todistr = "MinExtrVal", data = GBSG2)
mlt_GBSG2_1 <- mlt(ctm_GBSG2_1, data = GBSG2, maxit = 3000, scale = TRUE, check = FALSE)

coxph_GBSG2_1 <- coxph(fm_GBSG2, data = GBSG2)

cf <- coef(coxph_GBSG2_1)
cbind(coxph = cf, mlt = coef(mlt_GBSG2_1)[names(cf)])
cbind(coxph = sqrt(diag(vcov(coxph_GBSG2_1))), 
      mlt = sqrt(diag(vcov(mlt_GBSG2_1)))[names(cf)])
@

AFT Models

<<GBSG2-2>>=
ly <- log_basis(GBSG2y, ui = "increasing")
ctm_GBSG2_2 <- ctm(ly, shift = as.basis(fm_GBSG2[-2L], data = GBSG2, negative = TRUE), todistr = "MinExtrVal")
mlt_GBSG2_2 <- mlt(ctm_GBSG2_2, data = GBSG2, fixed = c("log(y)" = 1), scale = TRUE, check = FALSE)

survreg_GBSG2_2 <- survreg(fm_GBSG2, data = GBSG2, dist = "exponential")
phreg_GBSG2_2 <- phreg(fm_GBSG2, data = GBSG2, dist = "weibull", shape = 1)

logLik(mlt_GBSG2_2)
logLik(survreg_GBSG2_2)
logLik(phreg_GBSG2_2)

cbind(survreg = coef(survreg_GBSG2_2)[names(cf)], 
      phreg = coef(phreg_GBSG2_2)[names(cf)], 
      mlt = coef(mlt_GBSG2_2)[names(cf)])

cbind(survreg = sqrt(diag(vcov(survreg_GBSG2_2))[names(cf)]), 
      phreg = sqrt(diag(vcov(phreg_GBSG2_2))[names(cf)]),
      mlt = sqrt(diag(vcov(mlt_GBSG2_2)))[names(cf)])
@

<<GBSG2-3>>=
mlt_GBSG2_3 <- mlt(ctm_GBSG2_2, data = GBSG2, scale = TRUE, check = FALSE)

survreg_GBSG2_3 <- survreg(fm_GBSG2, data = GBSG2, dist = "weibull")
phreg_GBSG2_3 <- phreg(fm_GBSG2, data = GBSG2, dist = "weibull")

logLik(mlt_GBSG2_3)
logLik(survreg_GBSG2_3)
logLik(phreg_GBSG2_3)

cbind(survreg = coef(survreg_GBSG2_3)[names(cf)] / survreg_GBSG2_3$scale, 
      phreg = coef(phreg_GBSG2_3)[names(cf)], 
      mlt = coef(mlt_GBSG2_3)[names(cf)])
@

\paragraph{Truncated Regression: Tobin's Data}

Tobit, see truncreg

<<tobin>>=
data("tobin", package = "survival")
ttobin <- subset(tobin, durable > 0)
(truncreg_tobin <- truncreg(durable ~ age + quant, data = ttobin))

ttobin$durable <- R(ttobin$durable, tleft = 0)
b_d <- as.basis(~ durable, data = tobin, ui = matrix(c(0, 1), nr  = 1), ci = 0)
ctm_tobin <- ctm(b_d, shift = ~ age + quant, data = tobin, todistr = "Normal")
mlt_tobin <- mlt(ctm_tobin, data = ttobin, scale = TRUE, check = FALSE)

logLik(truncreg_tobin)
logLik(mlt_tobin)	

-coef(mlt_tobin)[c("(Intercept)", "age", "quant")] / coef(mlt_tobin)["durable"]
@

\paragraph{Robust Regression: Boston Housing Data}

lm; Boston Housing
<<BostonHousing>>=
data("BostonHousing", package = "mlbench")
lm_BH <- lm(medv ~ ., data = BostonHousing)
BostonHousing$medvc <- with(BostonHousing, Surv(medv, medv < 50))
mvar <- numeric_var("medvc", support = c(10.0, 40.0), bounds = c(0, Inf))
B_m <- Bernstein_basis(mvar, order = 6, ui = "increasing")
fm_BH <- medvc ~ crim + zn + indus + chas + nox + rm + age + 
    dis + rad + tax + ptratio + b + lstat
ctm_BH <- ctm(B_m, shift = fm_BH[-2L], data = BostonHousing, 
              todistr = "Normal")
mlt_BH <- mlt(ctm_BH, data = BostonHousing, scale = TRUE, 
                        check = FALSE)
@

\begin{figure}
\begin{center}
<<BostonHousing-plot, echo = FALSE, results = "hide", fig.width = 6, fig.height = 4>>=
q <- 3:52
m <- predict(lm_BH, data = BostonHousing)
s <- summary(lm_BH)$sigma
d <- sapply(m, function(m) pnorm(q, mean = m, sd = s))
nd <- expand.grid(q = q, lp = m)
nd$d <- c(d)

pfun <- function(x, y, z, subscripts, at, ...) {
    panel.contourplot(x, y, z, subscripts, at = 1:9/10, ...)
    panel.xyplot(x = m, y = BostonHousing$medv, pch = 20,   
                 col = rgb(.1, .1, .1, .1),  ...)   
}
p1 <- contourplot(d ~ lp + q, data = nd, panel = pfun, xlab = "Linear predictor", ylab = "Observed", main = "Normal Linear Model")

d <- predict(mlt_BH, newdata = BostonHousing, q = q, type="distribution")
lp <- c(predict(mlt_BH, newdata = BostonHousing, q = 0, terms = "bshift"))
nd <- expand.grid(q = q, lp = -lp)
nd$d <- c(d)
pfun <- function(x, y, z, subscripts, at, ...) {
    panel.contourplot(x, y, z, subscripts, at = 1:9/10, ...)
    panel.xyplot(x = -lp, y = BostonHousing$medv, pch = 20, 
                 col = rgb(.1, .1, .1, .1), ...)   
}
p2 <- contourplot(d ~ lp + q, data = nd, panel = pfun, xlab = "Linear predictor", ylab = "Observed", main = "Linear Transformation Model")
grid.arrange(p1, p2, nrow = 1)
@
\caption{Boston Housing. Predicted vs.~observed for the normal linear model (left) and the linear
         transformation model with smooth baseline transformation (right).\label{fig:BostonHousing-plot}}
\end{center}
\end{figure}


\subsection{Stratified Linear Transformation Models}

\begin{eqnarray*}
\Prob(\rY \le \ry | \text{stratum} = s, \rX = \rx) = \pZ(\h(\ry | s, \rx)) =
\pZ(\h(\ry | s) - \rx^\top \shiftparm) = \pZ(\basisyx(\ry, s, \rx)^\top \parm)
\basisyx = (\basisy^\top \otimes \basisx_\text{stratum}^\top, -\basisx_\text{shift}^\top)^\top                                       
\end{eqnarray*}

\subsubsection{Discrete Response}

\paragraph{Categorical Data Analysis: Chinese Survey}

Non-proportional odds model.

<<CHFLS-3, cache = TRUE>>=
b_health <- as.basis(~ R_health - 1, data = CHFLS)
ctm_CHFLS_3 <- ctm(b_happy, interacting = b_health, todist = "Logistic")
mlt_CHFLS_3 <- mlt(ctm_CHFLS_3, data = CHFLS, scale = TRUE,
                   check = FALSE, maxit = 5000)
predict(mlt_CHFLS_3, newdata = data.frame(R_health = sort(unique(CHFLS$R_health))), 
        q = sort(unique(CHFLS$R_happy)), type = "distribution")
@

With covariates

<<CHFLS-4, cache = TRUE>>=
ctm_CHFLS_4 <- ctm(b_happy, interacting = b_health, shift = b_R, todist = "Logistic")
mlt_CHFLS_4 <- mlt(ctm_CHFLS_4, data = CHFLS, scale = TRUE,
                   check = FALSE)
cftest(mlt_CHFLS_4)
@


\subsubsection{Continuous Response}

\paragraph{Survival Analysis: GBSG-2 Trial}

Estimate survivor function separately for the two treatment regimes

\begin{eqnarray*}
(\pMEV, (\bern{5}(\ry)^\top \otimes
       (\I(\text{hormonal therapy}), 1 - \I(\text{hormonal therapy})))^\top, c(\parm_1, \parm_2))
\end{eqnarray*}

<<GBSG2-4>>=
### two separate transformation functions
b_horTh <- as.basis(~ horTh - 1, data = GBSG2, remove_intercept = FALSE)
ctm_GBSG2_4 <- ctm(B_GBSG2y, interacting = b_horTh, todistr = "MinExtrVal")
mlt_GBSG2_4 <- mlt(ctm_GBSG2_4, data = GBSG2, check = FALSE)
@

\begin{figure}
\begin{center}
<<GBSG2-strata-plot, echo = FALSE, results = "hide">>=
nd <- expand.grid(s <- mkgrid(mlt_GBSG2_4, 100))
nd$mlt_S <- c(predict(mlt_GBSG2_4, newdata = s, type = "survivor"))
nd$KM_S <- unlist(predict(prodlim(Surv(time, cens) ~ horTh, data = GBSG2), 
     	             newdata = data.frame(horTh = s$horTh), times = s$y))
plot(nd$y, nd$mlt_S, ylim = c(0, 1), xlab = "Survival time (days)",
     ylab = "Probability", type = "n", las = 1)
with(subset(nd, horTh == "no"), lines(y, mlt_S, col = "grey", lty = 2))
with(subset(nd, horTh == "yes"), lines(y, mlt_S, lty = 2))
with(subset(nd, horTh == "no"), lines(y, KM_S, type = "s", col = "grey"))
with(subset(nd, horTh == "yes"), lines(y, KM_S, type = "s"))
legend("bottomright", lty = c(1, 1, 2, 2), col = c("black", "grey", "black", "grey"),
       legend = c("hormonal therapy, KM", "no hormonal therapy, KM", 
                  "hormonal therapy, MLT", "no hormonal therapy, MLT"), bty = "n", cex = .6)
@
\caption{GBSG-2 Trial. Estimated survivor functions 
         by the most likely transformation model (MLT) and the Kaplan-Meier (KM) estimator in the two 
         treatment groups. The plot reproduces
         Figure~4 (left panel) in \cite{Hothorn_Moest_Buehlmann_2016}. \label{GBSG2-strata-plot}}
\end{center}
\end{figure}

In a second step, we allowed a treatment-specific baseline hazard functions
for estimating the age effect
in the model 
\begin{eqnarray*}
(\pMEV, (\bern{5}(\ry)^\top \otimes
       (\I(\text{hormonal therapy}), 1 - \I(\text{hormonal therapy})),
       \text{age})^\top, (\parm_1, \beta)).
\end{eqnarray*}


<<GBSG2-5>>=
ctm_GBSG2_5 <- ctm(B_GBSG2y, interacting = b_horTh, shifting = ~ age, data = GBSG2, todistr = "MinExtrVal")
(mlt_GBSG2_5 <- mlt(ctm_GBSG2_5, data = GBSG2, check = FALSE, scale = TRUE))

coxph_GBSG2_5 <- coxph(Surv(time, cens) ~ age + strata(horTh), data = GBSG2)

cf <- coef(coxph_GBSG2_5)
cbind(coxph = cf, mlt = coef(mlt_GBSG2_5)[names(cf)])
cbind(coxph = sqrt(diag(vcov(coxph_GBSG2_5))),
      mlt = sqrt(diag(vcov(mlt_GBSG2_5)))[names(cf)])
@

\subsection{Conditional Transformation Models}

\begin{eqnarray*}
\basisyx = (\basisy_1^\top \otimes (\basisx_1^\top,\dots, \basisx_J^\top), -\basisx_\text{shift}^\top)
\end{eqnarray*}

<<ctm-fun, echo = FALSE>>=
printfun(ctm)
@

\subsubsection{Discrete Response}

\paragraph{Categorical Data Analysis: Chinese Survey}

Estimate the deviation of the conditional distribution of happiness
compared to the baseline distribution of happiness with very poor health.

<<CHFLS-5, cache = TRUE>>=
contrasts(CHFLS$R_health) <- "contr.treatment"
b_health <- as.basis(~ R_health, data = CHFLS)
ctm_CHFLS_5 <- ctm(b_happy, interacting = b_health, todist = "Logistic")
mlt_CHFLS_5 <- mlt(ctm_CHFLS_5, data = CHFLS, check = FALSE, scale = TRUE, maxit = 10000)
predict(mlt_CHFLS_5, newdata = data.frame(R_health = sort(unique(CHFLS$R_health))), 
        q = sort(unique(CHFLS$R_happy)), type = "distribution")
cftest(mlt_CHFLS_5)

ctm_CHFLS_6 <- ctm(b_happy, interacting = b_R, todist = "Logistic")  
mlt_CHFLS_6 <- mlt(ctm_CHFLS_6, data = CHFLS, scale = TRUE,
                   check = FALSE, maxit = 5000)
cftest(mlt_CHFLS_6)

ctm_CHFLS_7 <- ctm(b_happy, interacting = c(h = b_health, R = b_R), todist = "Logistic")  
mlt_CHFLS_7 <- mlt(ctm_CHFLS_7, data = CHFLS, scale = TRUE,
                   check = FALSE, maxit = 5000)
cftest(mlt_CHFLS_7)
@

\subsubsection{Continuous Response}

\paragraph{Survival Analysis: GBSG-2 Trial}

The Cox model $(\pMEV, (\bern{5}^\top, \I(\text{hormonal
therapy}))^\top, \parm)$ implements the transformation function $\h(\ry |
\text{treatment}) = \bern{5}(\ry)^\top \parm_1 + \I(\text{hormonal therapy})
\beta$ where $\bern{5}^\top \parm_1$ is the log-cumulative baseline hazard function
parameterised by a Bernstein polynomial and $\beta \in \RR$ is the
log-hazard ratio of hormonal therapy. 

<<GBSG2-6>>=
B_GBSG2y_5 <- Bernstein_basis(var = GBSG2y, order = 5, ui = "incre")
ctm_GBSG2_6 <- ctm(B_GBSG2y_5, shift = ~ horTh, data = GBSG2, todistr = "MinExtrVal")
mlt_GBSG2_6 <- mlt(ctm_GBSG2_6, data = GBSG2, check = FALSE)
@

This is the classical Cox model with one treatment parameter $\beta$ 
but fully parameterised baseline transformation function which was fitted by the exact
log-likelihood under five linear constraints. The model assumes proportional hazards, an assumption whose
appropriateness we wanted to assess using the non-proportional hazards model
$(\pMEV, (\bern{5}^\top \otimes (1, \I(\text{hormonal therapy})))^{\top}, \parm)$ with
transformation function 
\begin{eqnarray*}
\h(\ry | \text{treatment}) = \bern{5}(\ry)^\top \parm_1 + \I(\text{hormonal therapy}) \bern{5}(\ry)^\top \parm_2. 
\end{eqnarray*}
The function $\bern{5}^\top \parm_2$ is the time-varying treatment effect
and can be interpreted as the deviation, on the scale of the transformation
function, induced by the hormonal therapy.  Under the null hypothesis of no
treatment effect, we would expect $\parm_2 \equiv \bold{0}$.  This monotone
deviation function adds five linear constraints to the model. 
Figure~\ref{GBSG2-deviation-plot} shows the time-varying treatment effect
$\bern{5}^\top \hat{\parm}_2$, together with a $95\%$ confidence band
computed from the joint normal distribution of $\hat{\parm}_2$ for a grid
over time as described by \cite{Hothorn_Bretz_Westfall_2008}; 
the method is
much simpler than other methods for inference on time-varying
effects \citep[for example][]{Sun_Sundaram_Zhao_2009}.  The $95\%$
confidence interval around the log-hazard ratio $\hat{\beta}$ is plotted in
addition and since the latter is fully covered by the confidence band for
the time-varying treatment effect there is no reason to question the
treatment effect computed under the proportional hazards assumption.

<<GBSG2-7>>=
b_horTh <- as.basis(~ horTh, data = GBSG2)
ctm_GBSG2_7 <- ctm(B_GBSG2y_5, interacting = b_horTh, todistr = "MinExtrVal")
mlt_GBSG2_7 <- mlt(ctm_GBSG2_7, data = GBSG2, check = FALSE)
@

\begin{figure}
\begin{center} 
<<GBSG2-deviation-plot, echo = FALSE, results = "hide">>=

s <- mkgrid(mlt_GBSG2_7, 15)
s$y <- s$y[s$y > 100 & s$y < 2400]
nd <- expand.grid(s)
K <- model.matrix(ctm_GBSG2_7, data = nd)
Kyes <- K[nd$horTh == "yes",]
Kyes[,1:6] <- 0  
gh <- glht(parm(coef(mlt_GBSG2_7), vcov(mlt_GBSG2_7)), Kyes)
ci <- confint(gh)
coxy <- s$y

K <- matrix(0, nrow = 1, ncol = length(coef(mlt_GBSG2_6)))
K[,length(coef(mlt_GBSG2_6))] <- 1
ci2 <- confint(glht(mlt_GBSG2_6, K))

plot(coxy, ci$confint[, "Estimate"], ylim = range(ci$confint), type = "n",
     xlab = "Survival time (days)", ylab = "Transformation deviation", las = 1)
polygon(c(coxy, rev(coxy)), c(ci$confint[,"lwr"], rev(ci$confint[, "upr"])),
        border = NA, col = rgb(.1, .1, .1, .1))
lines(coxy, ci$confint[, "Estimate"], lty = 1, lwd = 1)
lines(coxy, rep(ci2$confint[,"Estimate"], length(coxy)), lty = 2, lwd = 1) 
lines(coxy, rep(0, length(coxy)), lty = 3)
polygon(c(coxy[c(1, length(coxy))], rev(coxy[c(1, length(coxy))])),
        rep(ci2$confint[,c("lwr", "upr")], c(2, 2)),
        border = NA, col = rgb(.1, .1, .1, .1))
legend("bottomright", lty = 1:2, lwd = 1, legend = c("time-varying treatment effect",
       "time-constant log-hazard ratio"), bty = "n", cex = .6)
@
\caption{GBSG-2 Trial. Verification of proportional hazards: The log-hazard ratio $\hat{\beta}$
         (dashed line) with $95\%$ confidence interval (dark grey) is fully 
         covered by a $95\%$ confidence band for the time-varying treatment effect (light grey,
         the estimate is the solid line)
         computed from a non-proportional hazards model. 
         The plot reproduces
         Figure~4 (right panel) in \cite{Hothorn_Moest_Buehlmann_2016}. \label{GBSG2-deviation-plot}}
\end{center}
\end{figure}   

In a second step, we allowed an age-varying treatment effect 
in the model $(\pMEV, (\bern{5}(\ry)^\top \otimes
       (\I(\text{hormonal therapy}), 1 - \I(\text{hormonal therapy}))
       \otimes \bernx{5}(\text{age})^\top)^\top, \parm)$. For both treatment
groups, we estimated a conditional transformation function of survival time
$\ry$ given age parameterised as the tensor basis of two Bernstein bases. Each of the
two basis functions comes with $5 \times 6$ linear constraints, so the model
was fitted under $60$ linear constraints. 
Figure~\ref{fig:GBSG2-8-plot} allows an assessment of the prognostic and
predictive properties of age.  As the surivor functions were clearly larger
under hormonal treatment for all patients, the positive treatment effect
applied to all patients.  However, the size of the treatment effect varied
greatly.  For women younger than $30$, the effect was most pronounced and
levelled-off a little for older patients.  In general, the survival times were
longest for women between $40$ and $60$ years old.  Younger women suffered
the highest risk; for women older than $60$ years, the risk started to
increase again.  This effect was shifted towards younger women by the
application of hormonal treatment.

<<GBSG2-8, echo = TRUE, cache = TRUE>>=
B_age <- Bernstein_basis(order = 3, var = numeric_var("age", support = range(GBSG2$age)))
b_horTh <- as.basis(~ horTh - 1, data = GBSG2)
ctm_GBSG2_8 <- ctm(B_GBSG2y_5, interacting = b(horTh = b_horTh, age = B_age), todistr = "MinExtrVal")
mlt_GBSG2_8  <- mlt(ctm_GBSG2_8, data = GBSG2, check = FALSE)
@

\begin{figure}[t]
\begin{center}
<<GBSG2-8-plot, echo = FALSE, fig.width = 6>>=
nlev <- c(no = "without hormonal therapy", yes = "with hormonal therapy")
levels(nd$horTh) <- nlev[match(levels(nd$horTh), names(nlev))]
s <- mkgrid(mlt_GBSG2_8, 100)
nd <- expand.grid(s)
nd$s <- c(predict(mlt_GBSG2_8, newdata = s, type = "survivor"))
contourplot(s ~ age + y | horTh, data = nd, at = 1:9 / 10,
            ylab = "Survival time (days)", xlab = "Age (years)",
            scales = list(x = list(alternating = c(1, 1))))
@
\caption{GBSG-2 Trial. Prognostic and predictive effect of age. The contours depict the
         conditional survivor functions given treatment and age of the patient. 
         The plot reproduces
         Figure~5 in \cite{Hothorn_Moest_Buehlmann_2016}.
         \label{fig:GBSG2-8-plot}}
\end{center}
\end{figure}

\paragraph{Quantile Regression: Head Circumference}

The Fourth Dutch Growth Study \citep{Fredriks_Buuren_Burgmeijer_2000} 
is a
cross-sectional study on growth and development of the Dutch population
younger than $22$ years.  \cite{Stasinopoulos_Rigby_2007} fitted
a growth curve to head circumferences (HC) of $7040$ boys using a GAMLSS
model with a Box-Cox $t$ distribution describing the first four moments of
head circumference conditionally on age.  The model showed evidence of
kurtosis, especially for older boys.  We fitted the same growth curves by the
conditional transformation model $(\Phi, (\bern{3}(\text{HC})^\top \otimes
\bernx{3}(\text{age}^{1/3})^\top)^\top, \parm)$ by maximisation of the approximate
log-likelihood under $3 \times 4$ linear constraints. 

<<head, echo = TRUE, cache = TRUE>>=
data("db", package = "gamlss.data")
db$lage <- with(db, age^(1/3))
B_h <- Bernstein_basis(order = 3, ui = "incre",
                      var = numeric_var("head", support = quantile(db$head, c(.1, .9)), bounds = range(db$head)))
B_a <- Bernstein_basis(order = 3, ui = "none",
                      var = numeric_var("lage", support = quantile(db$lage, c(.1, .9)), bounds = range(db$lage)))
ctm_head <- ctm(B_h, interacting = B_a)
mlt_head <- mlt(ctm_head, data = db, maxit = 5000, check = FALSE, scale = TRUE)
pr <- expand.grid(s <- mkgrid(ctm_head, 100))
pr$p <- c(predict(mlt_head, newdata = s, type = "distribution"))
pr$lage <- pr$lage^3
pr$cut <- factor(pr$lage > 2.5)
levels(pr$cut) <- c("Age < 2.5 yrs", "Age > 2.5 yrs")
@

Figure~\ref{fig:head-plot} shows the
data overlaid with quantile curves obtained via inversion of the estimated
conditional distributions.  The figure very closely reproduces the growth
curves presented in Figure~16 of \cite{Stasinopoulos_Rigby_2007} and also
indicates a certain asymmetry towards older boys.

\begin{figure}[t]
\begin{center}
<<head-plot, echo = FALSE, fig.width = 6>>=
pfun <- function(x, y, z, subscripts, at, ...) {
    panel.contourplot(x, y, z, subscripts,
        at = c(0.4, 2, 10, 25, 50, 75, 90, 98, 99.6)/ 100, ...)
    panel.xyplot(x = db$age, y = db$head, pch = 20,
                 col = rgb(.1, .1, .1, .1), ...)
}
print(contourplot(p ~ lage + head | cut, data = pr, panel = pfun, region = FALSE,
            xlab = "Age (years)", ylab = "Head circumference (cm)",
            scales = list(x = list(relation = "free"))))
@
\caption{Head Circumference Growth. Observed head circumference and age for
         $7040$ boys with estimated quantile curves for
         $\tau = 0.04, 0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98, 0.996$.
         The plot reproduces
         Figure~3 in \cite{Hothorn_Moest_Buehlmann_2016}.
         \label{fig:head-plot}}
\end{center}
\end{figure}

\subsubsection{Count Response}

\paragraph{Count Regression: Tree Pipit Counts}

\cite{Mueller_Hothorn_2004} reported data on the number of tree pipits
\textit{Anthus trivialis}, a small passerine bird, counted on $86$ forest plots at a light gradient
ranging from open and sunny stands (small cover storey) to dense and dark
stands (large cover storey).  We modelled the conditional distribution of the
number of tree pipits at one plot given the cover storey at this plot 
by the transformation
model $(\Phi, (\basisy^\top \otimes \bernx{4}(\text{cover storey})^\top)^\top,
\parm)$, where $\basisy(y) = \evec_5(y + 1), y = 0, \dots, 4$; the model
was fitted under $4 \times 5$ linear constraints. In this
model for count data, the conditional distribution depends on
both the number of counted birds and the cover storey and the effect of
cover storey may change with different numbers of birds observed.  The left
panel of Figure~\ref{fig:treepipit-plot} depicts the observations and the
center panel shows the conditional distribution function evaluated for $0,
\dots, 5$ observed birds.  The conditional distribution function obtained
from a generalised additive Poisson (GAM) model with smooth mean effect of cover
storey is given in the right panel.  Despite some overfitting, this model is
more restrictive than our transformation model because one mean function determines the whole distribution
(the local minima of the conditional distributions as a function of cover storey were
constant in the right panel whereas they were shifted towards higher values of
cover storey in the center panel).

<<treepipit, echo = TRUE, cache = TRUE>>=
data("treepipit", package = "coin")

tmp <- treepipit
tmp$counts <- ordered(tmp$counts)

B_cs <- Bernstein_basis(order = 4, var = numeric_var("coverstorey", support = 1:110))
B_c <- as.basis(~ counts, data = tmp, remove_intercept = TRUE,
                contrasts.arg = list(counts = function(n)
                    contr.treatment(n, base = 6)),
                ui = diff(diag(5)), ci = rep(0, 4))

ctm_treepipit <- ctm(B_c, interacting = B_cs)
mlt_treepipit <- mlt(ctm_treepipit, data = tmp, maxit = 10000, check = FALSE)

s <- mkgrid(ctm_treepipit, 100)
s$counts <- s$counts[1:5]
nd <- expand.grid(s)
nd$p <- c(predict(mlt_treepipit, newdata = s, type = "distribution"))

### produce a table
(tpt <- xtabs(~ counts + coverstorey, data = treepipit))

### construct a data frame with frequencies
treepipit2 <- sapply(as.data.frame(tpt, stringsAsFactors = FALSE),
                     as.integer)

s <- mkgrid(ctm_treepipit, 10)
s$counts <- s$counts[1]
K <- model.matrix(ctm_treepipit, data = expand.grid(s))
#g <- glht(parm(coef(mod), vcov(mod)), linfct = K)
#confint(g)

library("mgcv")
pmod <- gam(counts ~ s(coverstorey), data = treepipit, 
            family = "poisson")
nd$lambda <- predict(pmod, newdata = nd, type = "response")
@

\begin{figure}[t]
\begin{center}
<<treepipit-plot, fig.width=6, fig.height=3, echo = FALSE>>=
layout(matrix(1:3, nr = 1))
par("mai" = par("mai") * c(1, .95, 1, .85))
xlim <- range(treepipit[, "coverstorey"]) * c(0.98, 1.05)
xlab <- "Cover storey"
ylab <- "Number of tree pipits (TP)"
### scatterplot again; plots are proportional to number of plots
plot(counts ~ coverstorey, data = treepipit2, cex = sqrt(Freq),
     ylim = c(-.5, 5), xlab = xlab, ylab = ylab, col = "darkgrey", 
     xlim = xlim, las = 1, main = "Observations")

plot(c(0, 110), c(0, 1), type = "n", xlab = xlab, ylab = "Conditional probability",
     xlim = xlim, las = 1, main = "MLT")
with(subset(nd, counts == "0"), lines(coverstorey, p, lty = 1))
with(subset(nd, counts == "1"), lines(coverstorey, p, lty = 2))
with(subset(nd, counts == "2"), lines(coverstorey, p, lty = 3))
with(subset(nd, counts == "3"), lines(coverstorey, p, lty = 4))
with(subset(nd, counts == "4"), lines(coverstorey, p, lty = 5))
abline(h = 1, lty = 6)
legend("bottomright", lty = 1:6, legend = c(expression(TP == 0),
                                            expression(TP <= 1),
                                            expression(TP <= 2),
                                            expression(TP <= 3),
                                            expression(TP <= 4),
                                            expression(TP <= 5)), bty = "n")

plot(c(0, 110), c(0, 1), type = "n", xlab = xlab, ylab = "Conditional probability",
     xlim = xlim, las = 1, main = "GAM")
with(subset(nd, counts == "0"), lines(coverstorey, ppois(0, lambda), lty = 1))
with(subset(nd, counts == "1"), lines(coverstorey, ppois(1, lambda), lty = 2))
with(subset(nd, counts == "2"), lines(coverstorey, ppois(2, lambda), lty = 3))
with(subset(nd, counts == "3"), lines(coverstorey, ppois(3, lambda), lty = 4))
with(subset(nd, counts == "4"), lines(coverstorey, ppois(4, lambda), lty = 5))
abline(h = 1, lty = 6)
@
\caption{Tree Pipit Counts. Observations (left panel, the size of the points is
         proportional to the number of observations) and estimated conditional distribution
         of number of tree pipits given cover storey by the most likely transformation model (MLT, center panel)
         and a generalised additive Poisson model (function \cmd{gam} in package \pkg{mgcv}, 
         GAM, right panel). The plot reproduces
         Figure~7 in \cite{Hothorn_Moest_Buehlmann_2016}. \label{fig:treepipit-plot}}
\end{center}
\end{figure}


\section{Estimating Most Likely Transformations} \label{sec:mlt}


For a given transformation function $h$, the likelihood contribution of a
datum $\esAY = (\ubar{\ry},\bar{\ry}] \in \sAY$ is defined in terms of the
distribution function \citep{Lindsey_1996}: 
\begin{eqnarray*} 
\lik(\h | \rY \in \esAY) := \int_{\esAY} \dY(y | \h) d\measureY(y) =
\pZ(\h(\bar{\ry})) - \pZ(\h(\ubar{\ry})).
\end{eqnarray*}
This ``exact'' definition of the likelihood applies to most practically interesting
situations and, in particular, allows discrete and (conceptually) continuous as well as
censored or truncated observations $\esAY$. For a discrete response $\ry_k$ we have $\bar{\ry} = \ry_k$ and $\ubar{\ry}
= \ry_{k -1}$ such that $\lik(\h | \rY = \ry_k) = \dY(\ry_k | \h) =
\pZ(\h(\bar{\ry})) - \pZ(\h(\ubar{\ry}))$.  For absolutely continuous random
variables $\rY$ we always practically observe an imprecise datum $(\ubar{\ry},\bar{\ry}]
\subset \RR$ and, for short intervals $(\ubar{\ry},\bar{\ry}]$, approximate
the exact likelihood $\lik(\h | \rY \in (\ubar{\ry},\bar{\ry}])$ by the term
$(\bar{\ry} - \ubar{\ry}) \dY(\ry | \h)$ or simply $\dY(\ry | \h)$ with $\ry
= (\ubar{\ry} + \bar{\ry})/2$ \citep{Lindsey_1999}. This approximation only
works for relatively precise measurements, \ie short intervals. If longer intervals 
are observed, one speaks of ``censoring'' and relies on the exact definition
of the likelihood contribution instead of using the above approximation \citep{Klein_Moeschberger_2003}. 
In summary, the likelihood contribution of a conceptually ``exact
continuous'' or left, right or interval-censored continuous or discrete 
observation $(\ubar{\ry}, \bar{\ry}]$ is given by 
\begin{eqnarray*}
\lik(\h | \rY \in (\ubar{\ry}, \bar{\ry}]) \left\{ \begin{array}{ll}
    \approx \dZ(\h(\ry)) \h^\prime(\ry) & \ry = (\ubar{\ry} + \bar{\ry})/2 \in \samY \quad \text{```exact continuous'''}\\
    = 1 - \pZ(\h(\ubar{\ry})) & \ry \in (\ubar{\ry}, \infty) \cap \samY \quad \text{`right-censored'} \\
    = \pZ(\h(\bar{\ry})) & \ry \in (-\infty, \bar{\ry}] \cap \samY \quad \text{`left-censored'} \\
    = \pZ(\h(\bar{\ry})) - \pZ(\h(\ubar{\ry})) & \ry \in (\ubar{\ry},\bar{\ry}] \cap \samY \quad    \text{`interval-censored',}
\end{array} \right. 
\end{eqnarray*}
under the assumption of random censoring.
The likelihood is more complex under dependent censoring
\citep{Klein_Moeschberger_2003} but we do not elaborate on this issue further. 
The likelihood contribution $\lik(\h | \rY \in (\ry_k, \ry_{k-1}])$
of an ordered factor in category $\ry_k$ is equivalent to the term 
$\lik(\h | \rY \in (\ubar{\ry}, \bar{\ry}])$
contributed by an interval-censored observation $(\ubar{\ry},\bar{\ry}]$
when category $\ry_k$ was defined by the interval $(\ubar{\ry},\bar{\ry}]$. Thus,
the expression $\pZ(\h(\bar{\ry})) - \pZ(\h(\ubar{\ry}))$ for the likelihood
contribution reflects the equivalence of interval-censoring 
and categorisation at corresponding cut-off points. 

For truncated observations in the interval $(\ry_l, \ry_r] \subset \samY$,
the above likelihood contribution is defined in terms of the 
distribution function conditional on the truncation
\begin{eqnarray*}
\pY(\ry | \rY \in (\ry_l, \ry_r]) = \pZ(\h(\ry) | \rY \in (\ry_l, \ry_r]) = 
\frac{\pZ(\h(\ry))}{\pZ(\h(\ry_r)) - \pZ(\h(\ry_l))}  
\quad \forall \ry \in (\ry_l, \ry_r]
\end{eqnarray*}
and thus the likelihood contribution changes to \citep{Klein_Moeschberger_2003}
\begin{eqnarray*}
\frac{\lik(\h | \rY \in (\ubar{\ry}, \bar{\ry}])}{\pZ(\h(\ry_r)) -
\pZ(\h(\ry_l))} = \frac{\lik(\h | \rY \in (\ubar{\ry}, \bar{\ry}])}{\lik(\h
| \rY \in (\ry_l, \ry_r])} \quad \text{when } \ry_l < \ubar{\ry} < \bar{\ry} \le \ry_r.
\end{eqnarray*}
It is important to note that the likelihood is always \textit{defined} in
terms of a distribution function \citep{Lindsey_1999} and it therefore makes
sense to directly model the distribution function of interest.  The ability
to uniquely characterise this distribution function by the
transformation function $\h$ gives rise to the following definition of an
estimator $\hat{\h}_N$.

\begin{defn}[Most likely transformation]
Let $\esAY_1, \dots, \esAY_N$ denote an independent sample of possibly
censored or truncated observations from $\Prob_\rY$. The estimator
\begin{eqnarray*}
\hat{\h}_N := \argmax_{\tilde{\h} \in \hs} \sum_{i = 1}^{N} \log(\lik(\tilde{\h} |
\rY \in \esAY_i)) 
\end{eqnarray*}
is called the most likely transformation (MLT).
\end{defn}
Log-concavity of $\dZ$ ensures concavity of the log-likelihood (except 
when all observations are right-censored) and thus
existence and uniqueness of $\hat{\h}_N$.

We parameterise the transformation function $\h(\ry)$ as a linear function of its
basis-transformed argument $\ry$ using a basis function $\basisy: \samY
\rightarrow \RR^\dimparm$ such that $\h(\ry) = \basisy(\ry)^\top \parm,
\parm \in \RR^\dimparm$.  The choice of the basis function $\basisy$ is
problem-specific and will be discussed in Section~\ref{sec:appl}.  The
likelihood $\lik$ only requires evaluation of $\h$, and only an
approximation thereof using the Lebesgue density of ``exact continuous'' observations 
makes the evaluation of the 
first derivative of $\h(\ry)$ with respect to $\ry$ necessary.  In this
case, the derivative with respect to $\ry$ is given by
$\h^\prime(\ry) = \basisy^\prime(\ry)^\top \parm$ and we assume that
$\basisy^\prime$ is available.
In the following we write $\h = \basisy^\top
\parm$ and $\h^\prime = {\basisy^\prime}^\top \parm$ for the transformation
function and its first derivative omitting the argument $\ry$
and we assume that both functions are bounded away from $-\infty$ and
$\infty$. 

\begin{defn}[Maximum likelihood estimator]
\begin{eqnarray*}
\hat{\parm}_N := \argmax_{\parm \in \Theta} \sum_{i = 1}^N
\log(\lik(\basisy^\top \parm | \rY \in \esAY_i))
\end{eqnarray*}
\end{defn}

\section{Looking at Transformation Models} \label{sec:predict}

Based on the maximum likelihood estimator $\hat{\parm}_N$, we
define plug-in estimators of the most likely transformation function and
the corresponding estimator of our target distribution $\pY$ as 
$\hat{\h}_N := \basisy^\top \hat{\parm}_N$ and 
$\hatpY := \pZ \circ \hat{\h}_N$.


\section{Classical Likelihood Inference} \label{sec:asympt}


Because the problem of estimating an unknown distribution function is now
embedded in the maximum likelihood framework, the asymptotic analysis
benefits from standard results on the asymptotic behaviour of maximum likelihood
estimators.
We begin with deriving the score function and Fisher information.
The score contribution of an ``exact continuous'' 
observation $\ry = (\ubar{\ry} +
\bar{\ry})/2$ from an absolutely continuous distribution is approximated by the
gradient of the log-density
\begin{eqnarray}
\s(\parm | \rY \in (\ubar{\ry}, \bar{\ry}]) \approx
\frac{\partial \log(\dY(\ry | \parm))}{\partial \parm} & = & 
\frac{\partial \log(\dZ(\basisy(\ry)^\top \parm))) +
\log({\basisy^\prime(\ry)}^\top \parm)}{\partial \parm} \nonumber \\
& = & \basisy(\ry) \frac{\dZ^\prime(\basisy(\ry)^\top \parm)}{\dZ(\basisy(\ry)^\top \parm)}
    + \frac{\basisy^\prime(\ry)}{{\basisy^\prime(\ry)}^\top \parm}. \label{f:s_exact}
\end{eqnarray}
For an interval censored or discrete observation $\ubar{\ry}$ and
$\bar{\ry}$ (the constant terms $\pZ(\basisy(\pm \infty)^\top \parm) =
\pZ(\pm \infty) = 1$ or $0$ vanish) the score contribution is
\begin{eqnarray}
\s(\parm | \rY \in (\ubar{\ry}, \bar{\ry}]) & = & \frac{\partial \log(\lik(\basisy^\top \parm | \rY \in (\ubar{\ry},
\bar{\ry}]))}{\partial \parm} \nonumber \\
& = & \frac{\partial \log(\pZ(\basisy(\bar{\ry})^\top \parm) - \pZ(\basisy(\ubar{\ry})^\top \parm))}{\partial \parm}  \nonumber \\
& = & \frac{\dZ(\basisy(\bar{\ry})^\top \parm)\basisy(\bar{\ry}) - \dZ(\basisy(\ubar{\ry})^\top
\parm) \basisy(\ubar{\ry})}{\pZ(\basisy(\bar{\ry})^\top \parm) - \pZ(\basisy(\ubar{\ry})^\top
\parm)}. \label{f:s_interval}
\end{eqnarray}
For a truncated observation, the score function is $\s(\parm | \rY \in
(\ubar{\ry}, \bar{\ry}]) - \s(\parm | \rY \in (\ry_l, \ry_r])$.
%\begin{eqnarray*}
%\frac{\partial -\log[\pZ(\basisy(\ry_r)^\top \parm) - \pZ(\basisy(\ry_l)^\top
%\parm)]}{\partial \parm} & = &
%-\frac{\dZ(\basisy(\ry_r)^\top \parm)\basisy(\ry_r) - \dZ(\basisy(\ry_l)^\top \parm)\basisy(\ry_l)}
%     {\pZ(\basisy(\ry_r)^\top \parm) - \pZ(\basisy(\ry_l)^\top \parm)}
%\end{eqnarray*}
%has to be added to the score contribution $\s(\parm | \rY \in (\ubar{\ry},
%\bar{\ry}])$.

The contribution of an ``exact continuous''  observation $\ry$ from an
absolutely
continuous distribution to the Fisher information is approximately
\begin{eqnarray}
\mF(\parm | \rY \in (\ubar{\ry}, \bar{\ry}]) & \approx & 
-\frac{\partial^2 \log(\dY(\ry | \parm))}{\partial \parm
\partial \parm^\top} \nonumber \\
& = & - \left(
\basisy(\ry) \basisy(\ry)^\top \left\{
    \frac{\dZ^{\prime\prime}(\basisy(\ry)^\top \parm)}{\dZ(\basisy(\ry)^\top \parm)}
   -\left[\frac{\dZ^{\prime}(\basisy(\ry)^\top \parm)}{\dZ(\basisy(\ry)^\top \parm)}\right]^2\right\}
   - \frac{\basisy^\prime(\ry){\basisy^\prime(\ry)}^\top}{{(\basisy^\prime(\ry)}^\top\parm)^2}\right) \label{f:F_exact}
\end{eqnarray}
(NB: the weight to $\basisy(\ry) \basisy(\ry)^\top$ is constant one for
$\pZ = \Phi$). 
For a censored or discrete observation, we have the following
contribution to the Fisher information
\begin{eqnarray}
\mF(\parm | \rY \in (\ubar{\ry}, \bar{\ry}]) & = & -\frac{\partial^2 \log(\lik(\basisy^\top \parm |
\rY \in (\ubar{\ry}, \bar{\ry}]))}{\partial \parm \partial \parm^\top} \nonumber \\
& = & 
- \left\{\frac{\dZ^\prime(\basisy(\bar{\ry})^\top \parm)\basisy(\bar{\ry})\basisy(\bar{\ry})^\top -
      \dZ^\prime(\basisy(\ubar{\ry})^\top \parm) \basisy(\ubar{\ry}) \basisy(\ubar{\ry})^\top}
     {\pZ(\basisy(\bar{\ry})^\top \parm) - \pZ(\basisy(\ubar{\ry})^\top \parm)}
\right. \label{f:F_interval} \\
& &  \quad -\frac{[\dZ(\basisy(\bar{\ry})^\top \parm)\basisy(\bar{\ry}) - 
       \dZ(\basisy(\ubar{\ry})^\top \parm) \basisy(\ubar{\ry})] }
     {[\pZ(\basisy(\bar{\ry})^\top \parm) - \pZ(\basisy(\ubar{\ry})^\top \parm]^2} \times \nonumber \\
& & \left. \qquad      [\dZ(\basisy(\bar{\ry})^\top \parm)\basisy(\bar{\ry})^\top - 
       \dZ(\basisy(\ubar{\ry})^\top \parm) \basisy(\ubar{\ry})^\top]
\right\}. \nonumber
\end{eqnarray}
For a truncated observation, the Fisher information is given by
$\mF(\parm | \rY \in (\ubar{\ry}, \bar{\ry}]) - \mF(\parm | \rY \in (\ry_l,
\ry_r])$.

In particular, we can construct confidence
intervals and confidence bands for the conditional distribution function
from confidence intervals and bands for the linear functions $\basisy^\top
\parm$. We complete the theoretical part by formally defining the class of
transformation models.
\begin{defn}[Transformation model]
The triple $(\pZ, \basisy, \parm)$ is called transformation model.
\end{defn}


\section{Simulation-based Likelihood Inference} \label{sec:sim}


\section*{Computational Details}

<<funs, echo = FALSE, results='hide'>>=
if (file.exists("packages.bib")) file.remove("packages.bib")
pkgversion <- function(pkg) {
    pkgbib(pkg)
    packageDescription(pkg)$Version
}
pkgbib <- function(pkg) {
    x <- citation(package = pkg, auto = TRUE)[[1]]
    b <- toBibtex(x)
    b <- gsub("Buehlmann", "B{\\\\\"u}hlmann", b)
    b[1] <- paste("@Manual{pkg:", pkg, ",", sep = "")
    if (is.na(b["url"])) {
        b[length(b)] <- paste("   URL = {http://CRAN.R-project.org/package=",
                              pkg, "}", sep = "")
        b <- c(b, "}")
    }
    cat(b, sep = "\n", file = "packages.bib", append = TRUE)
}
pkg <- function(pkg)
    paste("\\\\pkg{", pkg, "} \\\\citep[version~",
          pkgversion(pkg), ",][]{pkg:", pkg, "}", sep = "")
pkg("gamlss")
pkg("MASS")
pkg("np")
pkg("prodlim")
mlt <- c("@Manual{pkg:mlt, \n
          title = {mlt: Most Likely Transformations},\n
          author = {Torsten Hothorn},\n
          url = {https://r-forge.r-project.org/projects/ctm/},\n
          year = {2016},\n
          note = {R package version 0.0-15, svn revision 313}}\n")
variables <- c("@Manual{pkg:variables, \n
          title = {variables: Variable Descriptions},\n
          author = {Torsten Hothorn},\n
          url = {https://r-forge.r-project.org/projects/ctm/},\n
          year = {2016},\n
          note = {R package version 0.0-15, svn revision 313}}\n")
basefun <- c("@Manual{pkg:basefun, \n
          title = {basefun: Infrastructure for Computing with Basis Functions},\n
          author = {Torsten Hothorn},\n
          url = {https://r-forge.r-project.org/projects/ctm/},\n
          year = {2016},\n
          note = {R package version 0.0-15, svn revision 313}}\n")
cat(mlt, sep = "\n", file = "packages.bib", append = TRUE)
cat(variables, sep = "\n", file = "packages.bib", append = TRUE)
cat(basefun, sep = "\n", file = "packages.bib", append = TRUE)
@

A reference implementation of most likely transformation models is available
in the \pkg{mlt} package \citep{pkg:mlt}. 
The spectral projected gradient method implemented
in the \cmd{spg} function of package \pkg{BB} \citep{Varadhan_Gilbert_2009}
was used for optimising the log-likelihood.
\citep{R}.

\bibliography{mlt,packages}

\begin{appendix}

\section{variables} \label{app:variables}

The \pkg{variables} packages \citep{pkg:variables} offers a small collection
of classes and methods for specifying and dealing with variable
descriptions.  The main purpose is to allow querying properties of variables
without having access to observations.  A variable description allows to
extract the name (\cmd{variable.name}), description (\cmd{desc}) and unit
(\cmd{unit}) of the variable along with the support (\cmd{support}) and
possible bounds (\cmd{bouds}) of the measurements. The \cmd{mkgrid} method
generates values from the variable description. The package differentiates between
factors, ordered factors and numeric variables.

\subsection{Unordered Factors}

Let eye color denote an unordered factor. The corresponding variable description is defined
by the name, description and levels of this factor:
<<variables-factor>>=
f_eye <- factor_var("eye", desc = "eye color", 
                    levels = c("blue", "brown", "green", "grey", "mixed"))
@
The properties of this factor are
<<variables-factor-methods>>=
variable.names(f_eye)
desc(f_eye)
units(f_eye)
support(f_eye)
bounds(f_eye)
is.bounded(f_eye)
@
and we can generate values via
<<variables-factor-mkgrid>>=
mkgrid(f_eye)
@

\subsection{Ordered Factors}

An ordered factor, temperature in categories is used here as an example, is defined as
in the unordered case:
<<variables-ordered>>=
o_temp <- ordered_var("temp", desc = "temperature", 
                      levels = c("cold", "lukewarm", "warm", "hot"))
@
and the only difference is that explicit bounds are known
<<variables-ordered-methods>>=
variable.names(o_temp)
desc(o_temp)
units(o_temp)
support(o_temp)
bounds(o_temp) 
is.bounded(o_temp)
mkgrid(o_temp)
@

\subsection{Discrete Numeric Variables}

Discrete numeric variables are defined by \cmd{numeric\_var} with integer-valued
\code{support} argument, here using age of a patient as example:
<<variables-fd>>=
v_age <- numeric_var("age", desc = "age of patient", 
                     unit = "years", support = 25:75)
@
The variable is bounded with finite support
<<variables-fd-methods>>=
variable.names(v_age)
desc(v_age)
units(v_age)
support(v_age)
bounds(v_age) 
is.bounded(v_age)
@
and the support is returned in
<<variables-fd-mkgrid>>=
mkgrid(v_age)
@

\subsection{Continuous Numeric Variables}

For conceptually continuous variables the \code{support} argument is a double vector
with two elements representing an interval with high density. The variable may or may not be bounded. 
For generating equally-spaced grids, \code{support + add} is used for unbounded variables or 
the corresponding finite boundaried if \code{add} is zero. Using the daytime temperature at Zurich
one could use
<<variables-c>>=
v_temp <- numeric_var("ztemp", desc = "Zurich daytime temperature", 
                      unit = "Celsius", support = c(-10.0, 35.0), 
                      add = c(-5, 5), bounds = c(-273.15, Inf))
@
where any model shall focus on temperatures between $-10$ and $35$ degrees Celsius.
<<variables-c-methods>>=
variable.names(v_temp)
desc(v_temp)
units(v_temp)
support(v_temp)
bounds(v_temp) 
is.bounded(v_temp)
mkgrid(v_temp, n = 20)
@

\subsection{Multiple Variables}

We can join multiple variable descriptions via \cmd{c}
<<variables-vars>>=
vars <- c(f_eye, o_temp, v_age, v_temp)
@
and all methods discussed above work accordingly
<<variables-vars-methods>>=
variable.names(vars)
desc(vars) 
units(vars)
support(vars)
bounds(vars)
is.bounded(vars)
mkgrid(vars, n = 20)
@
Calling
<<variables-vars-expand, eval = FALSE>>=
expand.grid(mkgrid(vars))
@
generates a \code{data.frame} with all possible values of the
variables and all combinations thereof.

\section{basefun} \label{app:basefun}

The \pkg{basefun} package \citep{pkg:basefun} implements Bernstein,
Legendre, log and polynomial basis functions.  In addition, facilities for
treating arbitrary model matrices as basis function are available.  Basis
functions can be joined columnwise using \cmd{c} or the box product can be
generated using \cmd{b}.  The definition of basis functions does not require
any actual observations, only variable descriptions (see
Appendix~\ref{app:variables}) are necessary.  Each basis offers
\cmd{model.matrix} and \cmd{predict} methods.  We illustrate how one can
set-up some of these basis functions in the following.

\subsection{Polynomial Basis}

For some positive variable $x$ we want to deal with the polynomial
$\alpha + \beta_1 x + \beta_3 x^3$ and first set-up a variable description
(see Appendix~\ref{app:variables}) and the basis function
<<basefun-polynom>>=
xvar <- numeric_var("x", support = c(0.1, pi), bound = c(0, Inf))
x <- as.data.frame(mkgrid(xvar, n = 20))
### set-up basis of order 3 ommiting the quadratic term
class(pb <- polynomial_basis(xvar, coef = c(TRUE, TRUE, FALSE, TRUE)))
@
The basis function \code{pb} is a \code{function}, therefore we can
evaluate the basis as
<<basefun-polynom-fun>>=
head(pb(x))
@
or, equivalently, using the corresponding \cmd{model.matrix} method
<<basefun-polynom-mm>>=
head(model.matrix(pb, data = x))
@
Evaluating the polynomial for some coefficients is done by the
\cmd{predict} method, which also allows derivatives to be computed
<<basefun-polynom-pred>>=
### evaluate polynomial defined by basis and coefficients
predict(pb, newdata = x, coef = c(1, 2, 0, 1.75))
### evaluate 1st derivative
predict(pb, newdata = x, coef = c(1, 2, 0, 1.75), deriv = c(x = 1L))
@

\subsection{Logarithmic Basis}

The monotone increasing logarithmic basis $\alpha + \beta \log(x)$ being 
subject to $\beta > 0$ is defined as
<<basefun-log>>=
### set-up log-basis with intercept for positive variable
class(lb <- log_basis(xvar, remove_intercept = FALSE, ui = "increasing"))
head(X <- model.matrix(lb, data = x))
@
The model matrix contains a \code{constraint} attribute
<<basefun-log-constr>>=
attr(X, "constraint")
@
where the linear constraints $\mA \parm \ge \mvec$ are represented by
a matrix $\mA$ (\code{ui}) and a vector $\mvec$ (\code{ci}). For 
$(\alpha, \beta) = (1, 2)$ the function and its derivative can be computed
as
<<basefun-log-pred>>=
predict(lb, newdata = x, coef = c(1, 2))
predict(lb, newdata = x, coef = c(1, 2), deriv = c(x = 1L))
@

\subsection{Bernstein Basis}
As an example, a monotone increasing Bernstein polynomial $\bern{3}$ can be defined and
evaluated as
<<basefun-Bernstein>>=
class(bb <- Bernstein_basis(xvar, order = 3, ui = "increasing"))
head(X <- model.matrix(bb, data = x))
### check constraints
cf <- c(1, 2, 2.5, 2.6)
(cnstr <- attr(X, "constraint"))
all(cnstr$ui %*% cf > cnstr$ci)
### evaluate Bernstein polynomial defined by basis and coefficients
predict(bb, newdata = x, coef = cf)
### evaluate first derivative 
predict(bb, newdata = x, coef = cf, deriv = c(x = 1))
@

\subsection{Model Matrices}

Model matrices are basis functions evaluated at some data. The \pkg{basefun}
package offers an \cmd{as.basis} method for \code{formula} objects which
basically represents unevaluated calls to \cmd{model.matrix} with two
additonal argments (\code{remove_intercept} removes the intercept
\textit{after} appropriate contrasts where computed and \code{negative}
multiplies the model matrix with $-1$).  Note that the \code{data} argument
does not need to be a \code{data.frame} with observations, a variable
description is sufficent:
<<basefun-as.basis>>=
iv <- as.vars(iris)
fb <- as.basis(~ Species + Sepal.Length + Sepal.Width,  data = iv,
               remove_intercept = TRUE, negative = TRUE, 
               contrasts.args =  list(Species = "contr.sum"))
class(fb)
head(model.matrix(fb, data = iris))
@

\subsection{Combining Bases}

Two (or more) basis functions can be concatenated by simply joining the corresponding
model matrices columnwise. If constraints $\mA_i \parm_i \ge \mvec_i$ are present for $i = 1, 2, \dots$ 
the overall constraints are given by a block-diagonal matrix $\mA = \text{blockdiag}(\mA_1, \mA_2, \dots)$,
$\parm = (\parm_1^\top, \parm_2^\top, \dots)^\top$ and $\mvec = (\mvec_1^\top, \mvec_2^\top, \dots)^\top$. As
an example we add a positive log-transformation to a Bernstein polynomial:
<<basefun-c>>=
class(blb <- c(bern = bb, log = log_basis(xvar, ui = "increasing")))
head(X <- model.matrix(blb, data = x))
attr(X, "constraint")
@

The box product of two basis functions is defined by the row-wise Kronecker product of the
corresponding model matrices but \textit{not} the Kronecker product of the model matrices (which
would result in a matrix with the same number of columns but squared number of rows).
<<basefun-b>>=
fb <- as.basis(~ g, data = factor_var("g", levels = LETTERS[1:2]))
### join them: we get one intercept and one deviation _function_
class(bfb <- b(bern = bb, f = fb))
nd <- expand.grid(mkgrid(bfb, n = 10))
### evaluate bases
head(X <- model.matrix(bfb, data = nd))
attr(X, "constraint")
bfb <- b(bern = bb, f = fb, sumconstr = TRUE)
### evaluate bases
head(X <- model.matrix(bfb, data = nd))
attr(X, "constraint")
@

\end{appendix}

\end{document}
