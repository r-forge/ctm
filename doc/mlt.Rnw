
\documentclass[article,nojss,shortnames]{jss}

%% packages
\usepackage{thumbpdf}
\usepackage{amsfonts,amstext,amsmath,amssymb,amsthm}
\usepackage{accents}
\usepackage{color}
\usepackage{rotating}
\usepackage{verbatim}
%% need no \usepackage{Sweave.sty}
%%\usepackage[nolists]{endfloat}

<<setup, echo = FALSE>>=
printfun <- function(fun) {
    f <- formals(fun)
    cf <- sapply(f, function(x) {
        if (is.call(x)) return(parse(text = x))
        if (is.null(x)) return("NULL")
        return(as.character(x))
    })
    fun <- as.name(match.call()[[2]])
    ret <- paste(fun, "(", paste(names(f), ifelse(cf == "", "", " = ") , 
          cf, collapse = ", ", sep = ""), ")", sep = "")
    cat(ret, "\n")
}
library("mlt")
library("survival")
@

\newcommand{\TODO}[1]{{\color{red} #1}}

\newcommand\Torsten[1]{{\color{blue}Torsten: ``#1''}}

% File with math commands etc.
\input{defs.tex}

\renewcommand{\thefootnote}{}

%% code commands
\newcommand{\Rclass}[1]{`\code{#1}'}
%% JSS
\author{Torsten Hothorn \\ Universit\"at Z\"urich}
\Plainauthor{Hothorn}

\title{\pkg{mlt}: Transformation Analysis in R} 
\Plaintitle{mlt: Transformation Analysis in R}
\Shorttitle{Transformation Analysis}

\Abstract{
tbd
}

\Keywords{transformation model, distribution regression, conditional
distribution function, conditional quantile function, censoring,
truncation}
\Plainkeywords{transformation model, distribution regression, conditional
distribution function, conditional quantile function, censoring,
truncation}

\Address{
  Torsten Hothorn\\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Universit\"at Z\"urich \\
  Hirschengraben 84, CH-8001 Z\"urich, Switzerland \\
  \texttt{Torsten.Hothorn@uzh.ch} \\
}

\begin{document}

\section{Introduction}

The history of statistics can be told as the story of great conceptual ideas
and contemporaneous computable approximations thereof.  As time went by, the
computationally inaccessible concept often vanished from the collective
consciousness of our profession and the approximation was taught and
understood as the real thing.  Least squares regression emerged from
Gau{\ss}' computational trick of changing Boscovics absolute to squared
error and it took more than 200 years for the original, and in many aspects
advantageous, concept to surface again under the name ``quantile
regression''.  This most prominent example of an idea got lost illustrates
the impact computable approximations had and still have on our understanding
of statistical methods and procedures.  In the early days of statistical
computing, implementations of such approximations were a challenge.  With
todays computing power and software infrastructure at our fingertips, our
duty shall be to go back to the original concepts and search for ways how to
reawake them for the benefit of a simpler understanding of statistical
models and concepts.

This paper describes an attempt to understand and unify a large class of
statistical models as models for distributions.  This sounds like an
implicitness, but do we really practice (in courses on applied statistics or
while talking to our subject-matter collaborators) what we preach in a
theory course?  Let's perform a small experiment: Pick, at random, a
statistics book from your book shelf and look-up how the general linear
model is introduced. Most probably you will find something not unlike
\begin{eqnarray*}
\rY = \alpha + \rx^\top \beta + \varepsilon, \quad \varepsilon \sim \ND(0, \sigma^2)
\end{eqnarray*}
where model interpretation relies on $\Ex(\rY | \rX = \rx) = \alpha + \rx^\top \beta$ and
one estimates the intercept $\alpha$ and the 
regression parameters $\beta$ by minimisation of the squared error 
$(\rY - \alpha - \rx^\top \beta)^2$. With some touch-up in notation, the model 
can be equivalently written as a model for a conditional distribution
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \Phi\left(\frac{\ry - \alpha - \rx^\top \beta}{\sigma}\right) \text{ or } (\rY | \rX = \rx) \sim \ND(\alpha + \rx^\top\beta, \sigma^2).
\end{eqnarray*}
This little change highlights that the model is, in fact, a model for a
conditional distribution and not just a model for a conditional mean. It also
stresses the fact that the variance $\sigma^2$ is a model parameter in its own right. The usual
treatment of $\sigma^2$ as a nuisance parameter only works when the likelihood 
is approximated by the density of the normal distribtion. Since we always observe
intervals $(\ubar{\ry}, \bar{\ry}]$ and never real numbers $\ry$, the exact likelihood
is
\begin{eqnarray*}
\Prob(\ubar{\ry} < \rY \le \bar{\ry} | \rX = \rx) = 
\Phi\left(\frac{\bar{\ry} - \alpha - \rx^\top \beta}{\sigma}\right) - \Phi\left(\frac{\ubar{\ry} - \alpha - \rx^\top \beta}{\sigma}\right)
\end{eqnarray*}
which requires simultaneous optimisation of all three model parameters $\alpha$, $\beta$ and $\sigma$ but
is exact also under other forms of random censoring. If we were going to reformulate the model
a little further to
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \Phi(\tilde{\alpha}_1 + \tilde{\alpha}_2 \ry - \rx^\top \tilde{\beta})
\end{eqnarray*}
with $\tilde{\alpha}_1 = -\alpha / \sigma, \tilde{\alpha}_2 = 1 / \sigma$ and $\tilde{\beta} = \beta / \sigma$
we see that the model is of the form
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h_\rY(\ry) - \rx^\top \tilde{\beta})
\end{eqnarray*}
with distribution function $\pZ = \Phi$ and linear transformation $\h_\rY(\ry) = \tilde{\alpha}_1 + \tilde{\alpha}_2 \ry$
such that $\Ex(\h_\rY(\rY) | \rX = \rx) = \rx^\top \beta$. If we now change $\pZ$ to the distribution
function of the minimum extreme value distribution and allow a non-linear monotone transformation
$\h_\rY$ we get
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = 1 - \exp(-\exp(\h_\rY(\ry) - \rx^\top \tilde{\beta}))
\end{eqnarray*}
which is the continuous proportional hazards, or Cox, model. From this point
of view, the linear and the Cox model are two instances of so-called linear
transformation models (a misleading name, because the transformation
$\h_\rY$ is non-linear in the latter case and only the shift $\rx^\top
\tilde{\beta}$ is linear in $\rx$).  It is now also obvious that the Cox
model has nothing to do with censoring, let alone survival times $\rY > 0$. 
It is a model for the conditional distribution of a continuous responses
$\rY \in \RR$ when it is appropriate to assume that the conditional hazard
function is scaled by $\exp(\rx^\top \tilde{\beta})$.  For both the linear
and the Cox model, application of the exact likelihood allows the models to
be fitted to imprecise, or ``censored'', observations $(\ubar{\ry},
\bar{\ry}]$.

The class of linear transformation models is a subclass of conditional 
transformation models. The conditional distribution function is then
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h(\ry | \rx))
\end{eqnarray*}
where the transformation function $\h$ depends on both $\ry$ and $\rx$. We 
describe how such models can be specified, fitted and analysed in \proglang{R}
using the \pkg{mlt} add-on package. Before we start looking at details,
we illustrate the workflow by means of an example from unconditional density 
estimation.

The duration of eruptions and the waiting time between eruptions of the Old
Faithful geyser in the Yellowstone national park became a standard benchmark
for non-parametric density estimation \citep[the original data were given
by][]{Azzalini_Bowman_1990}.  An unconditional density estimate for the
duration of the eruptions needs to deal with censoring because exact
duration times are only available for the day time measurements.  At night
time, the observations were either left-censored (``short'' eruption),
interval-censored (``medium'' eruption) or right-censored (``long''
eruption) as explained by \cite{Azzalini_Bowman_1990}.  This fact was widely
ignored in analyses of the Old Faithful data because most non-parametric
kernel techniques cannot deal with censoring. We fit the parameters
$\parm$ in the transformation model
\begin{eqnarray*}
\Prob(\rY \le \ry) = \Phi(\h(\ry)) = \Phi(\basisy(\ry)^\top \parm)
\end{eqnarray*}
by maximisation of the exact likelihood as follows. After loading package
\pkg{mlt} we specify the \code{duration} variable we are interested in
<<geyser-var, echo = TRUE>>=
dvar <- numeric_var("duration", support = c(1.0, 5.0), bound = c(0, Inf))
@
This abstract representation refers to a positive and conceptually
continuous variable \code{duration}.  We then set-up a basis function
$\basisy$ for this variable in the interval $[1, 5]$, in our case a monotone
increasing Bernstein polynomial of order eight
<<geyser-basis, echo = TRUE>>=                            
Bd <- Bernstein_basis(order = 8, var = dvar, ui = "incre")
@
The (in our case unconditional) transformation model is now fully described by
the parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$ and $\pZ = \Phi$
<<geyser-ctm, echo = TRUE>>=
md <- ctm(Bd, todistr = "Normal")
@
and we can generate a grid of duration times from
<<geyser-grid, echo = TRUE>>=
str(ndd <- mkgrid(md, 100))
@
Only after the model was specified we need to load the data containing the
\code{duration} variable as a \code{Surv} object: 
<<geyser-data, echo = TRUE>>=
data("geyser", package = "TH.data")
head(geyser)
@
The most likely transformation $\hat{\h}(\ry) = \basisy(\ry)^\top \hat{\parm}$ is 
now obtained from the maximum likelihood estimate $\hat{\parm}$ computed as
<<geyser-fit, echo = TRUE>>=
modd <- mlt(md, data = geyser, check = FALSE)
logLik(modd)
coef(modd)
vcov(modd)
@
and the model is best visualised in terms of the corresponding density
$\phi(\basisy(\ry)^\top \hat{\parm}) \basisy^\prime(\ry)^\top \hat{\parm}$
<<geyser-density, echo = TRUE>>=
ndd$d <- predict(modd, newdata = ndd, type = "density")
@
depicted in Figure~\ref{fig:geyser-plot}. The plot shows the well-known bimodal
distribution in a nice smooth way. Several things are quite unusual in this short example. 
First, the model was specified without reference to the actual observations, maybe 
with the exception of the domain of the Bernstein polynomial. Second, although the
model is fully parametric, the resulting density resembles a non-parametric flexibility.
Third, the exact likelihood as defined by the interval-censored observations, was used
to obtain the model. Fourth, no regularisation was necessary due to the monotonicity
constraint (implemented as linear constraints for maximum likelihood estimation) and thus
standard likelihood asymptotics work for $\hat{\parm}$. Fifth,
inspection of the parameter estimates is uninteresting, the model is better looked at
by means of the estimated distribution, density, quantile, hazard or cumulative 
hazard functions. Sixth, because the model is a model for a full distribution, we can 
easily draw random samples from the model and refit its parameters using the parametric
bootstrap. Seventh, all of this is not only possible theoretically but readily implemented
in package \pkg{mlt}. The only remaining question is ``Do all this nice properties
carry over to the conditional case, \ie to regression models?''. The answer to this
question is ``yes!'' and the rest of this paper describes the details following the
workflow sketched in this section.

\begin{figure}
\begin{center}
<<geyser-plot, echo = TRUE, fig = TRUE>>=
plot(d ~ duration, data = ndd, type = "l", ylab = "Density")
@
\caption{Estimated density for duration. \label{fig:geyser-plot}}
\end{center}
\end{figure}

 
\section{Specifying Transformation Models}

\subsection{Unconditional Transformation Models}

\begin{eqnarray*}
\Prob(\rY \le \ry) = \pZ(\h(\ry)) = \pZ(\basisy(\ry)^\top \parm) \\
\end{eqnarray*}

\subsection{Linear Transformation Models}

\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h(\ry | \rx)) = 
\pZ(\h(\ry) - \rx^\top \shiftparm) = \pZ(\basisyx(\ry, \rx)^\top \parm) = \pZ(\basisy(\ry)^\top \parm_1 - \rx^\top \shiftparm) \\ 
\basisyx = (\basisy^\top, -\basisx_\text{shift}^\top)^\top
\end{eqnarray*}

\subsection{Stratified Linear Transformation Models}

\begin{eqnarray*}
\Prob(\rY \le \ry | \text{stratum} = s, \rX = \rx) = \pZ(\h(\ry | s, \rx)) =
\pZ(\h(\ry | s) - \rx^\top \shiftparm) = \pZ(\basisyx(\ry, s, \rx)^\top \parm)
\basisyx = (\basisy^\top \otimes \basisx_\text{stratum}^\top, -\basisx_\text{shift}^\top)^\top                                       
\end{eqnarray*}

\subsection{Conditional Transformation Models}

\begin{eqnarray*}
\basisyx = (\basisy_1^\top \otimes (\basisx_1^\top,\dots, \basisx_J^\top), -\basisx_\text{shift}^\top)
\end{eqnarray*}

<<ctm-fun, echo = FALSE>>=
printfun(ctm)
@

\section{Estimating Most Likely Transformations}

\section{Looking at Transformation Models}

\section{Classical Likelihood Inference for Most Likely Transformations}

\section{Simulation Likelihood Inference for Most Likely Transformations}

\section*{Computational Details}

<<funs, echo = FALSE, results = hide>>=
if (file.exists("packages.bib")) file.remove("packages.bib")
pkgversion <- function(pkg) {
    pkgbib(pkg)
    packageDescription(pkg)$Version
}
pkgbib <- function(pkg) {
    x <- citation(package = pkg, auto = TRUE)[[1]]
    b <- toBibtex(x)
    b <- gsub("Buehlmann", "B{\\\\\"u}hlmann", b)
    b[1] <- paste("@Manual{pkg:", pkg, ",", sep = "")
    if (is.na(b["url"])) {
        b[length(b)] <- paste("   URL = {http://CRAN.R-project.org/package=",
                              pkg, "}", sep = "")
        b <- c(b, "}")
    }
    cat(b, sep = "\n", file = "packages.bib", append = TRUE)
}
pkg <- function(pkg)
    paste("\\\\pkg{", pkg, "} \\\\citep[version~",
          pkgversion(pkg), ",][]{pkg:", pkg, "}", sep = "")
pkg("gamlss")
mlt <- c("@Manual{pkg:mlt, \n
          title = {mlt: Most Likely Transformations},\n
          author = {Torsten Hothorn},\n
          url = {https://r-forge.r-project.org/projects/ctm/},\n
          year = {2015},\n
          note = {R package version 0.0-15, svn revision 313}}\n")
cat(mlt, sep = "\n", file = "packages.bib", append = TRUE)
@

A reference implementation of most likely transformation models is available
in the \pkg{mlt} package \citep{pkg:mlt}. 
The spectral projected gradient method implemented
in the \code{spg()} function of package \pkg{BB} \citep{Varadhan_Gilbert_2009}
was used for optimising the log-likelihood.
\citep{R}.

\bibliography{mlt,packages}

\begin{appendix}

\section{variables}

\section{basefun}

\end{appendix}

\end{document}
