
\documentclass[article,nojss,shortnames]{jss}

%% packages
\usepackage{thumbpdf}
\usepackage{amsfonts,amstext,amsmath,amssymb,amsthm}
\usepackage{accents}
\usepackage{color}
\usepackage{rotating}
\usepackage{verbatim}
%% need no \usepackage{Sweave.sty}
%%\usepackage[nolists]{endfloat}

<<setup, echo = FALSE, results = "hide", message = FALSE>>=
printfun <- function(fun) {
    f <- formals(fun)
    cf <- sapply(f, function(x) {
        if (is.call(x)) return(parse(text = x))
        if (is.null(x)) return("NULL")
        return(as.character(x))
    })
    fun <- as.name(match.call()[[2]])
    ret <- paste(fun, "(", paste(names(f), ifelse(cf == "", "", " = ") , 
          cf, collapse = ", ", sep = ""), ")", sep = "")
    cat(ret, "\n")
}
library("mlt")
library("survival")
library("lattice")
library("gridExtra")
library("grid") ### for textGrob
library("latticeExtra")
trellis.par.set(list(plot.symbol = list(col=1,pch=20, cex=0.7),
                     box.rectangle = list(col=1),
                     box.umbrella = list(lty=1, col=1),
                     strip.background = list(col = "white")))
ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)
library("colorspace")
library("multcomp")
@

\newcommand{\TODO}[1]{{\color{red} #1}}

\newcommand\Torsten[1]{{\color{blue}Torsten: ``#1''}}

% File with math commands etc.
\input{defs.tex}

\renewcommand{\thefootnote}{}

%% code commands
\newcommand{\Rclass}[1]{`\code{#1}'}
%% JSS
\author{Torsten Hothorn \\ Universit\"at Z\"urich}
\Plainauthor{Hothorn}

\title{\pkg{mlt}: Transformation Analysis in R} 
\Plaintitle{mlt: Transformation Analysis in R}
\Shorttitle{Transformation Analysis}

\Abstract{
tbd
}

\Keywords{transformation model, distribution regression, conditional
distribution function, conditional quantile function, censoring,
truncation}
\Plainkeywords{transformation model, distribution regression, conditional
distribution function, conditional quantile function, censoring,
truncation}

\Address{
  Torsten Hothorn\\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Universit\"at Z\"urich \\
  Hirschengraben 84, CH-8001 Z\"urich, Switzerland \\
  \texttt{Torsten.Hothorn@uzh.ch} \\
}

\begin{document}

\section{Introduction}

The history of statistics can be told as the story of great conceptual ideas
and contemporaneous computable approximations thereof.  As time went by, the
computationally inaccessible concept often vanished from the collective
consciousness of our profession and the approximation was taught and
understood as the real thing.  Least squares regression emerged from
Gau{\ss}' computational trick of changing Boscovics absolute to squared
error and it took more than 200 years for the original, and in many aspects
advantageous, concept to surface again under the name ``quantile
regression''.  This most prominent example of an idea got lost illustrates
the impact computable approximations had and still have on our understanding
of statistical methods and procedures.  In the early days of statistical
computing, implementations of such approximations were a challenge.  With
todays computing power and software infrastructure at our fingertips, our
duty shall be to go back to the original concepts and search for ways how to
reawake them for the benefit of a simpler understanding of statistical
models and concepts.

This paper describes an attempt to understand and unify a large class of
statistical models as models for distributions.  This sounds like an
implicitness, but do we really practice (in courses on applied statistics or
while talking to our subject-matter collaborators) what we preach in a
theory course?  Let's perform a small experiment: Pick, at random, a
statistics book from your book shelf and look-up how the general linear
model is introduced. Most probably you will find something not unlike
\begin{eqnarray*}
\rY = \alpha + \rx^\top \beta + \varepsilon, \quad \varepsilon \sim \ND(0, \sigma^2)
\end{eqnarray*}
where model interpretation relies on $\Ex(\rY | \rX = \rx) = \alpha + \rx^\top \beta$ and
one estimates the intercept $\alpha$ and the 
regression parameters $\beta$ by minimisation of the squared error 
$(\rY - \alpha - \rx^\top \beta)^2$. With some touch-up in notation, the model 
can be equivalently written as a model for a conditional distribution
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \Phi\left(\frac{\ry - \alpha - \rx^\top \beta}{\sigma}\right) \text{ or } (\rY | \rX = \rx) \sim \ND(\alpha + \rx^\top\beta, \sigma^2).
\end{eqnarray*}
This little change highlights that the model is, in fact, a model for a
conditional distribution and not just a model for a conditional mean. It also
stresses the fact that the variance $\sigma^2$ is a model parameter in its own right. The usual
treatment of $\sigma^2$ as a nuisance parameter only works when the likelihood 
is approximated by the density of the normal distribtion. Since we always observe
intervals $(\ubar{\ry}, \bar{\ry}]$ and never real numbers $\ry$, the exact likelihood
is
\begin{eqnarray*}
\Prob(\ubar{\ry} < \rY \le \bar{\ry} | \rX = \rx) = 
\Phi\left(\frac{\bar{\ry} - \alpha - \rx^\top \beta}{\sigma}\right) - \Phi\left(\frac{\ubar{\ry} - \alpha - \rx^\top \beta}{\sigma}\right)
\end{eqnarray*}
which requires simultaneous optimisation of all three model parameters $\alpha$, $\beta$ and $\sigma$ but
is exact also under other forms of random censoring. If we were going to reformulate the model
a little further to
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \Phi(\tilde{\alpha}_1 + \tilde{\alpha}_2 \ry - \rx^\top \tilde{\beta})
\end{eqnarray*}
with $\tilde{\alpha}_1 = -\alpha / \sigma, \tilde{\alpha}_2 = 1 / \sigma$ and $\tilde{\beta} = \beta / \sigma$
we see that the model is of the form
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h_\rY(\ry) - \rx^\top \tilde{\beta})
\end{eqnarray*}
with distribution function $\pZ = \Phi$ and linear transformation $\h_\rY(\ry) = \tilde{\alpha}_1 + \tilde{\alpha}_2 \ry$
such that $\Ex(\h_\rY(\rY) | \rX = \rx) = \rx^\top \beta$. If we now change $\pZ$ to the distribution
function of the minimum extreme value distribution and allow a non-linear monotone transformation
$\h_\rY$ we get
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = 1 - \exp(-\exp(\h_\rY(\ry) - \rx^\top \tilde{\beta}))
\end{eqnarray*}
which is the continuous proportional hazards, or Cox, model. From this point
of view, the linear and the Cox model are two instances of so-called linear
transformation models (a misleading name, because the transformation
$\h_\rY$ is non-linear in the latter case and only the shift $\rx^\top
\tilde{\beta}$ is linear in $\rx$).  It is now also obvious that the Cox
model has nothing to do with censoring, let alone survival times $\rY > 0$. 
It is a model for the conditional distribution of a continuous responses
$\rY \in \RR$ when it is appropriate to assume that the conditional hazard
function is scaled by $\exp(\rx^\top \tilde{\beta})$.  For both the linear
and the Cox model, application of the exact likelihood allows the models to
be fitted to imprecise, or ``censored'', observations $(\ubar{\ry},
\bar{\ry}]$.

The class of linear transformation models is a subclass of conditional 
transformation models. The conditional distribution function is then
\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h(\ry | \rx))
\end{eqnarray*}
where the transformation function $\h$ depends on both $\ry$ and $\rx$. We 
describe how such models can be specified, fitted and analysed in \proglang{R}
using the \pkg{mlt} add-on package. Before we start looking at details,
we illustrate the workflow by means of an example from unconditional density 
estimation.

The duration of eruptions and the waiting time between eruptions of the Old
Faithful geyser in the Yellowstone national park became a standard benchmark
for non-parametric density estimation \citep[the original data were given
by][]{Azzalini_Bowman_1990}.  An unconditional density estimate for the
duration of the eruptions needs to deal with censoring because exact
duration times are only available for the day time measurements.  At night
time, the observations were either left-censored (``short'' eruption),
interval-censored (``medium'' eruption) or right-censored (``long''
eruption) as explained by \cite{Azzalini_Bowman_1990}.  This fact was widely
ignored in analyses of the Old Faithful data because most non-parametric
kernel techniques cannot deal with censoring. We fit the parameters
$\parm$ in the transformation model
\begin{eqnarray*}
\Prob(\rY \le \ry) = \Phi(\h(\ry)) = \Phi(\basisy(\ry)^\top \parm)
\end{eqnarray*}
by maximisation of the exact likelihood as follows. After loading package
\pkg{mlt} we specify the \code{duration} variable we are interested in
<<geyser-var, echo = TRUE>>=
dvar <- numeric_var("duration", support = c(1.0, 5.0), bound = c(0, Inf))
@
This abstract representation refers to a positive and conceptually
continuous variable \code{duration}.  We then set-up a basis function
$\basisy$ for this variable in the interval $[1, 5]$, in our case a monotone
increasing Bernstein polynomial of order eight
<<geyser-basis, echo = TRUE>>=
Bd <- Bernstein_basis(order = 8, var = dvar, ui = "incre")
@
The (in our case unconditional) transformation model is now fully described by
the parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$ and $\pZ = \Phi$
<<geyser-ctm, echo = TRUE>>=
md <- ctm(Bd, todistr = "Normal")
@
and we can generate a grid of duration times from
<<geyser-grid, echo = TRUE>>=
str(ndd <- mkgrid(md, 100))
@
Only after the model was specified we need to load the data containing the
\code{duration} variable as a \code{Surv} object: 
<<geyser-data, echo = TRUE>>=
data("geyser", package = "TH.data")
head(geyser)
@
The most likely transformation $\hat{\h}(\ry) = \basisy(\ry)^\top \hat{\parm}$ is 
now obtained from the maximum likelihood estimate $\hat{\parm}$ computed as
<<geyser-fit, echo = TRUE>>=
mltd <- mlt(md, data = geyser, check = FALSE)
logLik(mltd)
coef(mltd)
vcov(mltd)
@
and the model is best visualised in terms of the corresponding density
$\phi(\basisy(\ry)^\top \hat{\parm}) \basisy^\prime(\ry)^\top \hat{\parm}$
<<geyser-density, echo = TRUE>>=
ndd$d <- predict(mltd, newdata = ndd, type = "density")
@
depicted in Figure~\ref{fig:geyser-plot}. The plot shows the well-known bimodal
distribution in a nice smooth way. Several things are quite unusual in this short example. 
First, the model was specified without reference to the actual observations, maybe 
with the exception of the domain of the Bernstein polynomial. Second, although the
model is fully parametric, the resulting density resembles a non-parametric flexibility.
Third, the exact likelihood as defined by the interval-censored observations, was used
to obtain the model. Fourth, no regularisation was necessary due to the monotonicity
constraint (implemented as linear constraints for maximum likelihood estimation) and thus
standard likelihood asymptotics work for $\hat{\parm}$. Fifth,
inspection of the parameter estimates is uninteresting, the model is better looked at
by means of the estimated distribution, density, quantile, hazard or cumulative 
hazard functions. Sixth, because the model is a model for a full distribution, we can 
easily draw random samples from the model and refit its parameters using the parametric
bootstrap. Seventh, all of this is not only possible theoretically but readily implemented
in package \pkg{mlt}. The only remaining question is ``Do all this nice properties
carry over to the conditional case, \ie to regression models?''. The answer to this
question is ``yes!'' and the rest of this paper describes the details following the
workflow sketched in this section.

\begin{figure}
\begin{center}
<<geyser-plot, echo = TRUE>>=
plot(d ~ duration, data = ndd, type = "l", ylab = "Density")
@
\caption{Estimated density for duration. \label{fig:geyser-plot}}
\end{center}
\end{figure}

 
\section{Specifying Transformation Models}

In this section we study a cascade of increasingly complex transformation models
and discuss how one can specify such models using the infrastructure provided
by the \pkg{mlt} package. We start with the simplest case of models for
unconditional distribution functions.

\subsection{Unconditional Transformation Models}

The distribution function on an at least ordered response $\rY$ is
defined in terms of a transformation function $\h$ and a distribution
function $\pZ$. The transformation function is parameterised in terms
of a basis function $\basisy$:
\begin{eqnarray*}
\Prob(\rY \le \ry) = \pZ(\h(\ry)) = \pZ(\basisy(\ry)^\top \parm).
\end{eqnarray*}
The triple $(\pZ, \basisy, \parm)$ fully defines the distribution of $\rY$ 
is called transformation model.
The choice of the basis function $\basisy$ depends on the measurement 
scale of $\rY$ and we can differentiate between the following situations.

\paragraph{Discrete Models for Categorical Responses}

For ordered categorical responses $\rY$ from a
finite sample space $\samY = \{\ry_1, \dots, \ry_K\}$ we assign one
parameter to each element of the sample space except $\ry_K$.  This
corresponds to the basis function $\basisy(\ry_k) = \evec_{K - 1}(k)$, where
$\evec_{K-1}(k)$ is the unit vector of length $K - 1$ with its
$k$th element being one.  The transformation function $\h$ is
\begin{eqnarray*}
\h(\ry_k) & = & \evec_{K - 1}(k)^\top \parm = \eparm_k \in \RR, \quad 1 \le k < K, \quad
\text{st} \quad \eparm_1 < \dots <
\eparm_{K - 1}
\end{eqnarray*}
with $\h(\ry_K) = \infty$,  and the unconditional distribution function of $\pY$ is
$\pY(\ry_k) = \pZ(\eparm_k)$. Note that monotonicity of $\h$ is guaranteed by the
$K - 2$ linear constraints $\eparm_2 - \eparm_1 > 0, \dots, \eparm_{K -1} -
\eparm_{K -2} > 0$ when constrained optimisation is performed.

For an ordered categorical variable at five levels with density $(.15, .25, .18, .23, .19)$
<<ctm-uncon-disc-var>>=
yvar <- ordered_var("y", levels = LETTERS[1:5])
(gy <- mkgrid(yvar))
dy <- c(.15, .25, .18, .23, .19)
@
we can specify the discrete basis function $\basisy$ as
<<ctm-uncon-disc-basis>>=
a <- as.basis(~ y, data = yvar, remove_intercept = TRUE,
              contrasts.arg = list(y = function(n)
                  contr.treatment(n, base = 5)),
              ui = diff(diag(4)), ci = rep(0, 3))
model.matrix(a, data = as.data.frame(gy))
@
The basis function evaluated at the levels of the factor can
be obtained from the \code{model.matrix} method, along with
the linear constraints imposed by this basis function.

The conditional transformation model (although being unconditional here) 
is defined using the \code{ctm} function.
<<ctm-uncon-disc-ctm>>=
my <- ctm(a)
@
An unfitted model can be set-up by a call to the fitting function
\code{mlt} with argument \code{dofit = FALSE}. A model with exactly
the density defined above only requires the corresponding coefficients
to be given to the resulting object
<<ctm-uncon-disc-mlt>>=
mlty <- mlt(my, data = as.data.frame(gy), dofit = FALSE)
coef(mlty) <- qnorm(cumsum(dy[-5L]))
@
Because the model is now fully defined, we can draw random samples
from this model and refit the model
<<ctm-uncon-disc-sim, message = FALSE>>=
ry <- do.call("c", simulate(mlty, nsim = 10000))
ry <- ordered(ry, levels = 1:nlevels(yvar), labels = levels(yvar))
nd <- data.frame(y = ry)
mltnd <- mlt(my, data = nd, check = FALSE)
@
The true and estimated distribution and density functions can 
now be compared
<<ctm-uncon-disc-output>>=
cumsum(dy)
c(pnorm(coef(mltnd)), 1)
predict(mltnd, q = gy, type = "distribution")
pnorm(confint(mltnd))
cbind(dy, diff(c(0, c(pnorm(coef(mltnd)), 1))))
@
Of course, the above exercise is an extremely cumbersome way
of estimating a discrete density those maximum likelihood estimator
simply is
<<ctm-uncon-table>>=
prop.table(table(ry))
@
but, as we will see in the rest of this paper, a generalisation
to the conditional case strongly relies on this parameterisation.

\paragraph{Continuous Models for Continuous Responses}

For continuous responses $\rY$ the
parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$, and thus also
$\hatpY$, should be smooth in $\ry$, so any polynomial or spline basis is a
suitable choice for $\basisy$.  We apply Bernstein polynomials \citep[for an
overview see][]{Farouki_2012} of order $M$
($\dimparm = M + 1$) defined on the interval $[\ubar{\imath}, \bar{\imath}]$ with
\begin{eqnarray*}
\bern{M}(\ry) & = & (M + 1)^{-1}(f_{\text{Be}(1, M + 1)}(\tilde{\ry}), \dots,
                            f_{\text{Be}(m, M - m + 1)}(\tilde{\ry}), \dots,
                            f_{\text{Be}(M + 1, 1)}(\tilde{\ry}))^\top \in \RR^{M + 1} \\
\h(\ry) & = & \bern{M}(\ry)^\top \parm =
              \sum_{m = 0}^{M} \eparm_m f_{\text{Be}(m + 1, M - m + 1)}(\tilde{\ry}) / (M + 1) \\
\h^\prime(\ry) & = & \bern{M}^\prime(\ry)^\top \parm =
              \sum_{m = 0}^{M - 1} (\eparm_{m + 1} - \eparm_m) f_{\text{Be}(m + 1, M - m)}(\tilde{\ry}) M /
((M + 1) (\bar{\imath} - \ubar{\imath}))
\end{eqnarray*}
where $\tilde{\ry} = (\ry -\ubar{\imath}) / (\bar{\imath} - \ubar{\imath}) \in [0,
1]$ and $f_{\text{Be}(m, M)}$ is the density of the Beta distribution with
parameters $m$ and $M$.  This choice is computationally attractive because
strict monotonicity can be formulated as a set of $M$ linear constraints on the
parameters $\eparm_m < \eparm_{m + 1}$ for all $m = 0, \dots, M$
\citep{Curtis_Ghosh_2011}.  Therefore, application of constrained optimisation guarantees
monotone estimates $\hat{\h}_N$. The basis contains an intercept.  

As an example, we want to estimate the unconditional distribution of
waiting times in the Old Faithful data. The Bernstein polynomial
for \code{waiting} is defined as
<<geyser-w>>=
wvar <- numeric_var("waiting", support = c(40.0, 100), bound = c(35, 115))
Bw <- Bernstein_basis(order = 8, var = wvar, ui = "incre")
@
The (in our case unconditional) transformation model is now fully described by
the parameterisation $\h(\ry) = \basisy(\ry)^\top \parm$ and $\pZ = \Phi$
<<geyser-w-ctm, echo = TRUE>>=
mw <- ctm(Bw, todistr = "Normal")
@
and we can generate a grid of waiting times from
<<geyser-w-grid, echo = TRUE>>=
str(ndw <- mkgrid(mw, 100))
@
The most likely transformation $\hat{\h}(\ry) = \basisy(\ry)^\top \hat{\parm}$ is 
now obtained from the maximum likelihood estimate $\hat{\parm}$ computed as
<<geyser-w-fit, echo = TRUE>>=
mltw <- mlt(mw, data = geyser, check = FALSE)
@
and we compare the estimated distribution function 
<<geyser-w-distribution, echo = TRUE>>=
ndw$d <- predict(mltw, newdata = ndw, type = "distribution")
@
with the empirical cumulative distribution function in Figure~\ref{fig:geyser-w-plot}.

\begin{figure}
\begin{center}
<<geyser-w-plot, echo = FALSE, cache = TRUE, height = 4>>=
layout(matrix(1:2, ncol = 2))
plot(ecdf(geyser$waiting), col = "grey", xlab = "Waiting times", main = "")
lines(ndw$waiting, ndw$d)
Bw <- Bernstein_basis(order = 40, var = wvar, ui = "incre")
mw <- ctm(Bw, todistr = "Normal")
mltw40 <- mlt(mw, data = geyser, check = FALSE)
ndw$d2 <- predict(mltw40, q = ndw$waiting, type = "distribution")
lines(ndw$waiting, ndw$d2, lty = 2)
legend("topleft", lty = 1:2, legend = c("M = 8", "M = 40"), bty = "n")
plot(ndw$waiting, predict(mltw, q = ndw$waiting, type = "density"), type = "l")
lines(ndw$waiting, predict(mltw40, q = ndw$waiting, type = "density"), lty = 2)
@
\caption{Waiting times. \label{fig:geyser-w-plot}}
\end{center}
\end{figure}

The question arises how the degree of the polynomial affects the estimated
distribution function.  On the one hand, the model $(\Phi, \bern{1}, \parm)$ only allows
linear transformation functions of a standard normal and $\pY$ is restricted to the
normal family.  On the other hand, $(\Phi, \bern{N - 1}, \parm)$ has one
parameter for each observation and $\hatpY$ is the non-parametric maximum
likelihood estimator $\text{ECDF}$ which, by the Glivenko-Cantelli lemma,
converges to $\pY$.  In this sense, we cannot choose $M$ ``too large''. This
is a consequence of the monotonicity constraint on the estimator
$\basisy^\top \hat{\parm}_N$ which, in this extreme case, just interpolates
the step-function $\pZ^{-1} \circ \text{ECDF}$.


\subsection{Linear Transformation Models}

\begin{eqnarray*}
\Prob(\rY \le \ry | \rX = \rx) = \pZ(\h(\ry | \rx)) = 
\pZ(\h(\ry) - \rx^\top \shiftparm) = \pZ(\basisyx(\ry, \rx)^\top \parm) = \pZ(\basisy(\ry)^\top \parm_1 - \rx^\top \shiftparm) \\ 
\basisyx = (\basisy^\top, -\basisx_\text{shift}^\top)^\top
\end{eqnarray*}

\paragraph{Discrete Response}

Proportional odds models

<<housing>>=
library("MASS")
polrhousing <- polr(Sat ~ Infl, weights = Freq, data = housing)
s <- as.basis(~ Infl, data = housing, remove_intercept = TRUE, 
              negative = TRUE)
r <- as.basis(~ Sat, data = housing, remove_intercept = TRUE,
              contrasts.arg = list(Sat = function(n) 
                  contr.treatment(n, base = 3)),
              ui = diff(diag(2)), ci = 0)
mhousing <- ctm(r, shift = s, todist = "Logi")
mlthousing <- mlt(mhousing, data = housing, weights = housing$Freq, check = FALSE)

logLik(polrhousing)
logLik(mlthousing)

cbind(polr = c(polrhousing$zeta, coef(polrhousing)), mlt = coef(mlthousing))

cbind(polr = sqrt(diag(vcov(polrhousing)))[c(3, 4, 1, 2)],
      mlt = sqrt(diag(vcov(mlthousing))))
@

\paragraph{Continuous Responses}

Cox model for GBSG-2

<<GBSG2-Cox, echo = TRUE>>=
data("GBSG2", package = "TH.data")
GBSG2$y <- with(GBSG2, Surv(time, cens))
dist <- "Min"
yvar <- numeric_var("y", support = c(100.0, max(GBSG2$time)), bounds = c(0, Inf))
### Cox model
By <- Bernstein_basis(var = yvar, order = 10, ui = "incre")
fmGBSG2 <- Surv(time, cens) ~ horTh + age + menostat + tsize + tgrade + pnodes + progrec + estrec
mGBSG2 <- ctm(By, shift = fmGBSG2[-2L], todistr = dist, data = GBSG2)
mltGBSG2 <- mlt(mGBSG2, data = GBSG2, maxit = 3000, scale = TRUE, check = FALSE)

coxphGBSG2 <- coxph(fmGBSG2, data = GBSG2)

cf <- coef(coxphGBSG2)
cbind(coxph = cf, mlt = coef(mltGBSG2)[names(cf)])
cbind(coxph = sqrt(diag(vcov(coxphGBSG2))), 
      mlt = sqrt(diag(vcov(mltGBSG2)))[names(cf)])
@

AFT Models

<<GBSG2-exponential>>=
ly <- log_basis(yvar, ui = "increasing")
mGBSG2 <- ctm(ly, shift = as.basis(fmGBSG2[-2L], data = GBSG2, negative = TRUE), todistr = dist)
mltGBSG2 <- mlt(mGBSG2, data = GBSG2, fixed = c("log(y)" = 1), scale = TRUE, check = FALSE)

survregGBSG2 <- survreg(fmGBSG2, data = GBSG2, dist = "exponential")
library("eha")
phregGBSG2 <- phreg(fmGBSG2, data = GBSG2, dist = "weibull", shape = 1)

logLik(mltGBSG2)
logLik(survregGBSG2)
phregGBSG2$loglik[2]

cbind(survreg = coef(survregGBSG2)[names(cf)], phreg = coef(phregGBSG2)[names(cf)], 
      mlt = coef(mltGBSG2)[names(cf)])

cbind(survreg = sqrt(diag(vcov(survregGBSG2))[names(cf)]), 
      phreg = sqrt(diag(phregGBSG2$var)[names(cf)]),
      mlt = sqrt(diag(vcov(mltGBSG2)))[names(cf)])
@

<<GBSG2-weibull>>=
mltGBSG2 <- mlt(mGBSG2, data = GBSG2, scale = TRUE, check = FALSE)

survregGBSG2 <- survreg(fmGBSG2, data = GBSG2, dist = "weibull")
library("eha")
phregGBSG2 <- phreg(fmGBSG2, data = GBSG2, dist = "weibull")

logLik(mltGBSG2)
logLik(survregGBSG2)
phregGBSG2$loglik[2]

cbind(survreg = coef(survregGBSG2)[names(cf)] / survregGBSG2$scale, phreg = coef(phregGBSG2)[names(cf)], 
      mlt = coef(mltGBSG2)[names(cf)])
@

Tobit, see truncreg


\subsection{Stratified Linear Transformation Models}

\begin{eqnarray*}
\Prob(\rY \le \ry | \text{stratum} = s, \rX = \rx) = \pZ(\h(\ry | s, \rx)) =
\pZ(\h(\ry | s) - \rx^\top \shiftparm) = \pZ(\basisyx(\ry, s, \rx)^\top \parm)
\basisyx = (\basisy^\top \otimes \basisx_\text{stratum}^\top, -\basisx_\text{shift}^\top)^\top                                       
\end{eqnarray*}

<<ovarian>>=
yvar <- numeric_var("time", support = range(ovarian$futime), bounds = c(0, Inf))
ovarian$time <- with(ovarian, Surv(futime, fustat))
ly <- log_basis(yvar, ui = "incre")
fmovarian <- Surv(futime, fustat) ~ age
movarian <- ctm(ly, interacting = as.basis(~ as.factor(rx) - 1, data = ovarian), 
                shift = as.basis(fmovarian[-2L], data = ovarian, remove_intercept = FALSE, negative = TRUE), todistr = dist)
mltovarian <- mlt(movarian, data = ovarian, scale = TRUE, check = FALSE)

survregovarian <- survreg(Surv(futime, fustat) ~ age + strata(rx), data=ovarian,
                          dist = "weibull")

logLik(mltovarian)
logLik(survregovarian)

cftest(mltovarian)
cftest(survregovarian)

@

\subsection{Conditional Transformation Models}

\begin{eqnarray*}
\basisyx = (\basisy_1^\top \otimes (\basisx_1^\top,\dots, \basisx_J^\top), -\basisx_\text{shift}^\top)
\end{eqnarray*}

<<ctm-fun, echo = FALSE>>=
printfun(ctm)
@

\section{Estimating Most Likely Transformations}

\section{Looking at Transformation Models}

\section{Classical Likelihood Inference for Most Likely Transformations}

\section{Simulation Likelihood Inference for Most Likely Transformations}

\section{Applications}

\paragraph{Quantile Regression: Head Circumference}

The Fourth Dutch Growth Study \citep{Fredriks_Buuren_Burgmeijer_2000} 
is a
cross-sectional study on growth and development of the Dutch population
younger than $22$ years.  \cite{Stasinopoulos_Rigby_2007} fitted
a growth curve to head circumferences (HC) of $7040$ boys using a GAMLSS
model with a Box-Cox $t$ distribution describing the first four moments of
head circumference conditionally on age.  The model showed evidence of
kurtosis, especially for older boys.  We fitted the same growth curves by the
conditional transformation model $(\Phi, (\bern{3}(\text{HC})^\top \otimes
\bernx{3}(\text{age}^{1/3})^\top)^\top, \parm)$ by maximisation of the approximate
log-likelihood under $3 \times 4$ linear constraints. 

<<head, echo = TRUE, cache = TRUE>>=
data("db", package = "gamlss.data")
db$lage <- with(db, age^(1/3))
B1 <- Bernstein_basis(order = 3, ui = "incre",
                      var = numeric_var("head", support = quantile(db$head, c(.1, .9)), bounds = range(db$head)))
B2 <- Bernstein_basis(order = 3, ui = "none",
                      var = numeric_var("lage", support = quantile(db$lage, c(.1, .9)), bounds = range(db$lage)))
m <- ctm(B1, interacting = B2)
mod <- mlt(m, data = db, maxit = 5000, check = FALSE, scale = TRUE)
pr <- expand.grid(s <- mkgrid(m, 100))
pr$p <- c(predict(mod, newdata = s, type = "distribution"))
pr$lage <- pr$lage^3
pr$cut <- factor(pr$lage > 2.5)
levels(pr$cut) <- c("Age < 2.5 yrs", "Age > 2.5 yrs")
@

Figure~\ref{fig:head-plot} shows the
data overlaid with quantile curves obtained via inversion of the estimated
conditional distributions.  The figure very closely reproduces the growth
curves presented in Figure~16 of \cite{Stasinopoulos_Rigby_2007} and also
indicates a certain asymmetry towards older boys.

\begin{figure}[t]
\begin{center}
<<head-plot, echo = FALSE>>=
pfun <- function(x, y, z, subscripts, at, ...) {
    panel.contourplot(x, y, z, subscripts,
        at = c(0.4, 2, 10, 25, 50, 75, 90, 98, 99.6)/ 100, ...)
    panel.xyplot(x = db$age, y = db$head, pch = 20,
                 col = rgb(.1, .1, .1, .1), ...)
}
print(contourplot(p ~ lage + head | cut, data = pr, panel = pfun, region = FALSE,
            xlab = "Age (years)", ylab = "Head circumference (cm)",
            scales = list(x = list(relation = "free"))))
@
\caption{Head Circumference Growth. Observed head circumference and age for
         $7040$ boys with estimated quantile curves for
         $\tau = 0.04, 0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98, 0.996$.
         \label{fig:head-plot}}
\end{center}
\end{figure}

\paragraph{Survival Analysis: German Breast Cancer Study Group-2 Trial} 

This prospective, controlled clinical trial on the treatment of node positive
breast cancer patients was conducted by the German Breast Cancer Study 
Group \citep[GBSG-2,][]{gbsg2:1994}.  Patients not older than $65$ years with
positive regional lymph nodes but no distant metastases were included in
the study.  Out of $686$ women, $246$ received hormonal therapy whereas the
control group of $440$ women did not receive hormonal therapy.  Additional
variables include age, menopausal status, tumor size, tumor grade, number of
positive lymph nodes, progesterone receptor and estrogen receptor.  The
right-censored recurrence-free survival time is the response variable of
interest.

The Cox model $(\pMEV, (\bern{5}^\top, \I(\text{hormonal
therapy}))^\top, \parm)$ implements the transformation function $\h(\ry |
\text{treatment}) = \bern{5}(\ry)^\top \parm_1 + \I(\text{hormonal therapy})
\beta$ where $\bern{5}^\top \parm_1$ is the log-cumulative baseline hazard function
parameterised by a Bernstein polynomial and $\beta \in \RR$ is the
log-hazard ratio of hormonal therapy. 




This is the classical Cox model with one treatment parameter $\beta$ 
but fully parameterised baseline transformation function which was fitted by the exact
log-likelihood under five linear constraints. The model assumes proportional hazards, an assumption whose
appropriateness we wanted to assess using the non-proportional hazards model
$(\pMEV, (\bern{5}^\top \otimes (1, \I(\text{hormonal therapy})))^{\top}, \parm)$ with
transformation function 
\begin{eqnarray*}
\h(\ry | \text{treatment}) = \bern{5}(\ry)^\top \parm_1 + \I(\text{hormonal therapy}) \bern{5}(\ry)^\top \parm_2. 
\end{eqnarray*}
The function $\bern{5}^\top \parm_2$ is the time-varying treatment effect
and can be interpreted as the deviation, on the scale of the transformation
function, induced by the hormonal therapy.  Under the null hypothesis of no
treatment effect, we would expect $\parm_2 \equiv \bold{0}$.  This monotone
deviation function adds five linear constraints to the model. We first
compared the fitted survivor functions obtained from the model including 
a time-varying treatment effect with the Kaplan-Meier
estimators in both treatment groups.  The left panel of
Figure~\ref{fig_5_logrank} shows a nicely smoothed version of the survivor
functions obtained from this transformation model.  The right
panel of Figure~\ref{fig_5_logrank} shows the time-varying treatment effect
$\bern{5}^\top \hat{\parm}_2$, together with a $95\%$ confidence band
computed from the joint normal distribution of $\hat{\parm}_2$ for a grid
over time as described by \cite{Hothorn_Bretz_Westfall_2008}; 
the method is
much simpler than other methods for inference on time-varying
effects \citep[for example][]{Sun_Sundaram_Zhao_2009}.  The $95\%$
confidence interval around the log-hazard ratio $\hat{\beta}$ is plotted in
addition and since the latter is fully covered by the confidence band for
the time-varying treatment effect there is no reason to question the
treatment effect computed under the proportional hazards assumption.

<<GBSG2-timevarying, echo = TRUE, cache = TRUE>>=
### two separate transformation functions
BhorTh <- as.basis(~ horTh - 1, data = GBSG2, remove_intercept = FALSE)
By <- Bernstein_basis(order = 5, ui = "incre", var = yvar)
m <- ctm(By, interacting = BhorTh, todistr = dist)
mod <- mlt(m, data = GBSG2, check = FALSE)
AIC(mod)

s <- mkgrid(mod, 100)
nd <- expand.grid(s)
nd$h <- c(predict(mod, newdata = s))
nd$p <- c(predict(mod, newdata = s, type = "distribution"))

### deviation transformation
BhorTh <- as.basis(~ horTh, data = GBSG2)
m <- ctm(By, interacting = BhorTh, todistr = dist)
mod <- mlt(m, data = GBSG2, check = FALSE)
AIC(mod)

s <- mkgrid(mod, 15)
s$y <- s$y[s$y > 100 & s$y < 2400]
nd <- expand.grid(s)
K <- model.matrix(m, data = nd)
Kyes <- K[nd$horTh == "yes",]
Kyes[,1:6] <- 0
gh <- glht(parm(coef(mod), vcov(mod)), Kyes)
ci <- confint(gh)

m <- ctm(By, shifting = ~ horTh, todistr = dist, data = GBSG2)
coxmod <- mlt(m, data = GBSG2, check = FALSE)
AIC(coxmod)

K <- matrix(0, nrow = 1, ncol = length(coef(coxmod)))
K[,length(coef(coxmod))] <- 1
ci2 <- confint(glht(coxmod, K))
coxy <- s$y

### 2 separate KM curves
s <- mkgrid(mod, 50)
s$y <- s$y[s$y > 100 & s$y < 2400]
coxnd <- expand.grid(s)
coxnd$p <- c(predict(coxmod, newdata = s, type = "survivor"))
library("prodlim")
coxnd$pp <- unlist(predict(prodlim(Surv(time, cens) ~ horTh, data = GBSG2), 
                 newdata = data.frame(horTh = s$horTh), times = s$y))
@

\begin{figure}[t]
\begin{center}
<<fig_5_logrank, echo = FALSE>>=
layout(matrix(1:2, nrow = 1))
par("mai" = par("mai") * c(1, .95, 1, .85))
plot(coxnd$y, coxnd$p, ylim = c(0, 1), xlab = "Survival time (days)",
     ylab = "Probability", type = "n", las = 1)
with(subset(coxnd, horTh == "no"), lines(y, p, col = "grey", lty = 2))
with(subset(coxnd, horTh == "yes"), lines(y, p, lty = 2))
with(subset(coxnd, horTh == "no"), lines(y, pp, type = "s", col = "grey"))
with(subset(coxnd, horTh == "yes"), lines(y, pp, type = "s"))
legend("bottomright", lty = c(1, 1, 2, 2), col = c("black", "grey", "black", "grey"),
       legend = c("hormonal therapy, KM", "no hormonal therapy, KM",
                  "hormonal therapy, MLT", "no hormonal therapy, MLT"), bty = "n", cex = .6)
plot(coxy, ci$confint[, "Estimate"], ylim = range(ci$confint), type = "n",
     xlab = "Survival time (days)", ylab = "Transformation deviation", las = 1)
polygon(c(coxy, rev(coxy)), c(ci$confint[,"lwr"], rev(ci$confint[, "upr"])),
        border = NA, col = rgb(.1, .1, .1, .1))
lines(coxy, ci$confint[, "Estimate"], lty = 1, lwd = 1)
lines(coxy, rep(ci2$confint[,"Estimate"], length(coxy)), lty = 2, lwd = 1)
lines(coxy, rep(0, length(coxy)), lty = 3)
polygon(c(coxy[c(1, length(coxy))], rev(coxy[c(1, length(coxy))])), 
        rep(ci2$confint[,c("lwr", "upr")], c(2, 2)), 
        border = NA, col = rgb(.1, .1, .1, .1))
legend("bottomright", lty = 1:2, lwd = 1, legend = c("time-varying treatment effect", 
       "time-constant log-hazard ratio"), bty = "n", cex = .6)

@
\caption{GBSG-2. Estimated survivor functions 
         by the most likely transformation model (MLT) and the Kaplan-Meier (KM) estimator in the two 
         treatment groups (left panel). 
         Verification of proportional hazards (right panel): The log-hazard ratio $\hat{\beta}$
         (dashed line) with $95\%$ confidence interval (dark grey) is fully 
         covered by a $95\%$ confidence band for the time-varying treatment effect (light grey,
         the estimate is the solid line)
         computed from a non-proportional hazards model. \label{fig_5_logrank}}
\end{center}
\end{figure}


In a second step, we allowed an age-varying treatment effect 
in the model $(\pMEV, (\bern{5}(\ry)^\top \otimes
       (\I(\text{hormonal therapy}), 1 - \I(\text{hormonal therapy}))
       \otimes \bernx{5}(\text{age})^\top)^\top, \parm)$. For both treatment
groups, we estimated a conditional transformation function of survival time
$\ry$ given age parameterised as the tensor basis of two Bernstein bases. Each of the
two basis functions comes with $5 \times 6$ linear constraints, so the model
was fitted under $60$ linear constraints. 
Figure~\ref{fig_6_GBSG2} allows an assessment of the prognostic and
predictive properties of age.  As the surivor functions were clearly larger
under hormonal treatment for all patients, the positive treatment effect
applied to all patients.  However, the size of the treatment effect varied
greatly.  For women younger than $30$, the effect was most pronounced and
levelled-off a little for older patients.  In general, the survival times were
longest for women between $40$ and $60$ years old.  Younger women suffered
the highest risk; for women older than $60$ years, the risk started to
increase again.  This effect was shifted towards younger women by the
application of hormonal treatment.

<<GBSG2-agevarying, echo = TRUE, cache = TRUE>>=
### age-varying treatment effect
Bage <- Bernstein_basis(order = 3, var = numeric_var("age", support = range(GBSG2$age)))
BhorTh <- as.basis(~ horTh - 1, data = GBSG2)

m <- ctm(By, interacting = b(horTh = BhorTh, age = Bage), todistr = dist)
mod <- mlt(m, data = GBSG2, check = FALSE)

s <- mkgrid(m, 100)
nd <- expand.grid(s)
nd$h <- c(predict(mod, newdata = s))
nd$d <- c(predict(mod, newdata = s, type = "density"))
nd$p <- c(predict(mod, newdata = s, type = "distribution"))
nd$l <- c(predict(mod, newdata = s, type = "hazard"))
@

\begin{figure}[t]
\begin{center}
<<fig_6_GBSG2, echo = FALSE>>=
nlev <- c(no = "without hormonal therapy", yes = "with hormonal therapy")
levels(nd$horTh) <- nlev[match(levels(nd$horTh), names(nlev))]
contourplot(I(1 - p) ~ age + y | horTh, data = nd, at = 1:9 / 10,
            ylab = "Survival time (days)", xlab = "Age (years)",
            scales = list(x = list(alternating = c(1, 1))))
@
\caption{GBSG-2. Prognostic and predictive effect of age. The contours depict the
         conditional survivor functions given treatment and age of the patient. 
         \label{fig_6_GBSG2}}
\end{center}
\end{figure}

Finally, we fitted the Cox model $(\pMEV, (\bern{5}^\top, \tilde{\rx}^\top)^\top,
\parm)$ to the data, where $\tilde{\rx}$ contains the treatment indicator
and all other variables in the transformation function $\h(\ry | \rx) =
\bern{5}(\ry)^\top \parm_1 + \tilde{\rx}^\top \shiftparm$.  In contrast to the
classical Cox model where only $\shiftparm$ is estimated by the partial
likelihood, we estimated all model parameters simultaneously under five linear constraints.  
The results obtained from the partial and the exact log-likelihood are practically
equivalent, Figure~\ref{fig_7_cox} shows the corresponding $z$-statistics for
$\hat{\shiftparm}$.

\paragraph{Count Regression: Tree Pipit Counts}

\cite{Mueller_Hothorn_2004} reported data on the number of tree pipits
\textit{Anthus trivialis}, a small passerine bird, counted on $86$ forest plots at a light gradient
ranging from open and sunny stands (small cover storey) to dense and dark
stands (large cover storey).  We modelled the conditional distribution of the
number of tree pipits at one plot given the cover storey at this plot 
by the transformation
model $(\Phi, (\basisy^\top \otimes \bernx{4}(\text{cover storey})^\top)^\top,
\parm)$, where $\basisy(y) = \evec_5(y + 1), y = 0, \dots, 4$; the model
was fitted under $4 \times 5$ linear constraints. In this
model for count data, the conditional distribution depends on
both the number of counted birds and the cover storey and the effect of
cover storey may change with different numbers of birds observed.  The left
panel of Figure~\ref{fig_8_treepipit} depicts the observations and the
center panel shows the conditional distribution function evaluated for $0,
\dots, 5$ observed birds.  The conditional distribution function obtained
from a generalised additive Poisson (GAM) model with smooth mean effect of cover
storey is given in the right panel.  Despite some overfitting, this model is
more restrictive than our transformation model because one mean function determines the whole distribution
(the local minima of the conditional distributions as a function of cover storey were
constant in the right panel whereas they were shifted towards higher values of
cover storey in the center panel).

<<treepipit, echo = TRUE, cache = TRUE>>=
data("treepipit", package = "coin")

tmp <- treepipit
tmp$counts <- ordered(tmp$counts)

Bcs <- Bernstein_basis(order = 4, var = numeric_var("coverstorey", support = 1:110))
Bc <- as.basis(~ counts, data = tmp, remove_intercept = TRUE,
              contrasts.arg = list(counts = function(n)
                  contr.treatment(n, base = 6)),
              ui = diff(diag(5)), ci = rep(0, 4))

m <- ctm(Bc, interacting = Bcs)
mod <- mlt(m, data = tmp, maxit = 10000, check = FALSE)

s <- mkgrid(m, 100)
s$counts <- s$counts[1:5]
nd <- expand.grid(s)
nd$p <- c(predict(mod, newdata = s, type = "distribution"))

### produce a table
(tpt <- xtabs(~ counts + coverstorey, data = treepipit))

### construct a data frame with frequencies
treepipit2 <- sapply(as.data.frame(tpt, stringsAsFactors = FALSE),
                     as.integer)

s <- mkgrid(m, 10)
s$counts <- s$counts[1]
K <- model.matrix(m, data = expand.grid(s))
#g <- glht(parm(coef(mod), vcov(mod)), linfct = K)
#confint(g)

library("mgcv")
pmod <- gam(counts ~ s(coverstorey), data = treepipit, 
            family = "poisson")
nd$lambda <- predict(pmod, newdata = nd, type = "response")
@

\begin{figure}[t]
\begin{center}
<<fig_8_treepipit, fig.width=6, fig.height=3, echo = FALSE>>=
layout(matrix(1:3, nr = 1))
par("mai" = par("mai") * c(1, .95, 1, .85))
xlim <- range(treepipit[, "coverstorey"]) * c(0.98, 1.05)
xlab <- "Cover storey"
ylab <- "Number of tree pipits (TP)"
### scatterplot again; plots are proportional to number of plots
plot(counts ~ coverstorey, data = treepipit2, cex = sqrt(Freq),
     ylim = c(-.5, 5), xlab = xlab, ylab = ylab, col = "darkgrey", 
     xlim = xlim, las = 1, main = "Observations")

plot(c(0, 110), c(0, 1), type = "n", xlab = xlab, ylab = "Conditional probability",
     xlim = xlim, las = 1, main = "MLT")
with(subset(nd, counts == "0"), lines(coverstorey, p, lty = 1))
with(subset(nd, counts == "1"), lines(coverstorey, p, lty = 2))
with(subset(nd, counts == "2"), lines(coverstorey, p, lty = 3))
with(subset(nd, counts == "3"), lines(coverstorey, p, lty = 4))
with(subset(nd, counts == "4"), lines(coverstorey, p, lty = 5))
abline(h = 1, lty = 6)
legend("bottomright", lty = 1:6, legend = c(expression(TP == 0),
                                            expression(TP <= 1),
                                            expression(TP <= 2),
                                            expression(TP <= 3),
                                            expression(TP <= 4),
                                            expression(TP <= 5)), bty = "n")

plot(c(0, 110), c(0, 1), type = "n", xlab = xlab, ylab = "Conditional probability",
     xlim = xlim, las = 1, main = "GAM")
with(subset(nd, counts == "0"), lines(coverstorey, ppois(0, lambda), lty = 1))
with(subset(nd, counts == "1"), lines(coverstorey, ppois(1, lambda), lty = 2))
with(subset(nd, counts == "2"), lines(coverstorey, ppois(2, lambda), lty = 3))
with(subset(nd, counts == "3"), lines(coverstorey, ppois(3, lambda), lty = 4))
with(subset(nd, counts == "4"), lines(coverstorey, ppois(4, lambda), lty = 5))
abline(h = 1, lty = 6)
@
\caption{Tree Pipit Counts. Observations (left panel, the size of the points is
         proportional to the number of observations) and estimated conditional distribution
         of number of tree pipits given cover storey by the most likely transformation model (MLT, center panel)
         and a generalised additive Poisson model (function \code{gam()} in package \pkg{mgcv}, 
         GAM, right panel). \label{fig_8_treepipit}}
\end{center}
\end{figure}

\section*{Computational Details}

<<funs, echo = FALSE, results='hide'>>=
if (file.exists("packages.bib")) file.remove("packages.bib")
pkgversion <- function(pkg) {
    pkgbib(pkg)
    packageDescription(pkg)$Version
}
pkgbib <- function(pkg) {
    x <- citation(package = pkg, auto = TRUE)[[1]]
    b <- toBibtex(x)
    b <- gsub("Buehlmann", "B{\\\\\"u}hlmann", b)
    b[1] <- paste("@Manual{pkg:", pkg, ",", sep = "")
    if (is.na(b["url"])) {
        b[length(b)] <- paste("   URL = {http://CRAN.R-project.org/package=",
                              pkg, "}", sep = "")
        b <- c(b, "}")
    }
    cat(b, sep = "\n", file = "packages.bib", append = TRUE)
}
pkg <- function(pkg)
    paste("\\\\pkg{", pkg, "} \\\\citep[version~",
          pkgversion(pkg), ",][]{pkg:", pkg, "}", sep = "")
pkg("gamlss")
mlt <- c("@Manual{pkg:mlt, \n
          title = {mlt: Most Likely Transformations},\n
          author = {Torsten Hothorn},\n
          url = {https://r-forge.r-project.org/projects/ctm/},\n
          year = {2015},\n
          note = {R package version 0.0-15, svn revision 313}}\n")
cat(mlt, sep = "\n", file = "packages.bib", append = TRUE)
@

A reference implementation of most likely transformation models is available
in the \pkg{mlt} package \citep{pkg:mlt}. 
The spectral projected gradient method implemented
in the \code{spg()} function of package \pkg{BB} \citep{Varadhan_Gilbert_2009}
was used for optimising the log-likelihood.
\citep{R}.

\bibliography{mlt,packages}

\begin{appendix}

\section{variables}

\section{basefun}

\end{appendix}

\end{document}
